{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc02e9e6",
   "metadata": {},
   "source": [
    "# Hybrid recommender\n",
    "I did this one, just because I got intrested in how it works, so I thought, I'd try to make one myself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf1f93",
   "metadata": {},
   "source": [
    "## Plan:\n",
    "* Load and merge the data on WineID. Analyze features using ydata profiling.\n",
    "* Load the data, format columns to the correct types like:\n",
    "    * Grapes and Harmonize (string list) -> (python list)  \n",
    "    !!! This step is required in future always when loading this columns from csv.\n",
    "    * Vintage (str) -> numeric. I just simply replace N.V.(non-vintage) with 0 and then turn whole column to integer type.\n",
    "    * Datetime to a proper pd.datetime type\n",
    "* Filter datasets to keep only wines and users with >=5 ratings.\n",
    "* Merge datasets on WineID. Final size is 21,013,536 rows x 15 columns\n",
    "* Columns used:\n",
    "    * **WineID**: Integer. The wine primary key identification;\n",
    "    * **WineName**: String. The textual wine identification presented in the label;\n",
    "    * **Type**: String. The categorical type classification: Red, white or rosé for still wines, gasified sparkling or dessert for sweeter and fortified wines. Dessert/Port is a subclassification for liqueur dessert wines;\n",
    "    * **Elaborate**: String. Categorical classification between varietal or assemblage/blend. The most famous blends are also considered, such as * Bordeaux red and white blend, Valpolicella blend and Portuguese red and white blend;\n",
    "    * **Grapes**: String list. It contains the grape varieties used in the wine elaboration. The original names found have been kept;\n",
    "    * **Harmonize**: String list. It contains the main dishes set that pair with the wine item. These are provided by producers but openly recommended on the internet by sommeliers and even consumers;\n",
    "    * **ABV**: Float. The alcohol by volume (ABV) percentage. According to [1], the value shown on the label may vary, and a tolerance of 0.5% per 100 volume is allowed, reaching 0.8% for some wines;\n",
    "    * **Body**: String. The categorical body classification: Very light-bodied, light-bodied, medium-bodied, full-bodied or very full-bodied based on wine viscosity [37];\n",
    "    * **Acidity**: String. The categorical acidity classification: Low, medium, or high, based on potential hydrogen (pH) score [38];\n",
    "    * **Country**: String. The categorical origin country identification of the wine production (ISO-3166);\n",
    "    * **RegionName**: String. The textual wine region identification. The appellation region name was retained when identified;\n",
    "    * **WineryName**: String. The textual winery identification;\n",
    "    * **UserID**: Integer. The sequential key without identifying the user's private data;\n",
    "    * **Vintage**: String. A rated vintage year or the abbreviation \"N.V.\" referring to \"non-vintage\";\n",
    "    * **Date**: String. Datetime in the format YYYY-MM-DD hh:mm:ss informing when it was rated by the user. It can be easily converted to other formats.\n",
    "    * **Rating**(**Target variable**): Float. It contains the 5-stars (1–5) rating value ⊂ {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5} performed by the user;\n",
    "* Columns dropped:\n",
    "    * **RegionID** and **RatingID** - since they are just unique IDs and not descriptive for the recommender. We care only about users IDs and wines IDs.\n",
    "    * **Code** - since it's the same meaning as **Country** column. Either one can be selected.\n",
    "    * **Vintages** - since it's just lists of possible vintages and we already have a Vintage column with the exact value(year or 0 for non-vintage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c11f69",
   "metadata": {},
   "source": [
    "#### Used ChatGPT for speeding up the process, since we have a large dataset, and also for beautifying outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ee3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack, csr_matrix, issparse, lil_matrix, save_npz\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dcacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter of string lists into Python lists\n",
    "# (e.g. \"['a', 'b', 'c']\" → [a, b, c])\n",
    "def parse_list_col(s):\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Converter of 'N.V.' to 0, so column is numeric\n",
    "def parse_vintage(s):\n",
    "    return 0 if s == 'N.V.' else int(s)\n",
    "\n",
    "# Time-based split of a DataFrame by user, could be replaced later with other method\n",
    "# (e.g. 80% train, 20% test) based on the date column\n",
    "def user_time_split(df, date_col='Date', split_ratio=0.8):\n",
    "    train_parts, val_parts = [], []\n",
    "    for uid, group in df.groupby('UserID', sort=False):\n",
    "        group = group.sort_values(date_col)\n",
    "        i = int(len(group) * split_ratio)\n",
    "        train_parts.append(group.iloc[:i])\n",
    "        val_parts.append(group.iloc[i:])\n",
    "    return pd.concat(train_parts), pd.concat(val_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the wines data and parse string lists into Python lists\n",
    "print(\"▶ Reading wines metadata…\")\n",
    "wines = pd.read_csv(\n",
    "    './data/XWines_Full_100K_wines.csv',\n",
    "    usecols=['WineID', 'Type', 'Elaborate', 'ABV', 'Body', 'Acidity', 'RegionName', 'WineryName', 'Grapes','Harmonize','Country'],\n",
    "    converters={\n",
    "        'Grapes':    parse_list_col,\n",
    "        'Harmonize': parse_list_col\n",
    "    }\n",
    ")\n",
    "print(f\"   ✓ Loaded wines: {len(wines):,} rows\")\n",
    "\n",
    "# Read the ratings data and parse 'N.V.' into 0 (Vitage to numeric), parse dates\n",
    "print(\"▶ Reading ratings…\")\n",
    "ratings = pd.read_csv(\n",
    "    './data/XWines_Full_21M_ratings.csv',\n",
    "    usecols=['UserID','WineID','Date','Vintage','Rating'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "print(f\"   ✓ Loaded ratings: {len(ratings):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532745ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Noise reduction\n",
    "\n",
    "# Filter out wines with fewer than 5 ratings \n",
    "print(\"▶ Filtering wines with ≥5 ratings…\")\n",
    "wine_counts = ratings['WineID'].value_counts()\n",
    "good_wines  = wine_counts[wine_counts >= 5].index\n",
    "ratings     = ratings[ratings['WineID'].isin(good_wines)]\n",
    "print(f\"   ✓ Ratings remaining after wine filter: {len(ratings):,} rows\")\n",
    "\n",
    "# Filter out users with fewer than 5 reviews\n",
    "print(\"▶ Filtering users with ≥5 reviews…\")\n",
    "user_counts = ratings['UserID'].value_counts()\n",
    "good_users  = user_counts[user_counts >= 5].index\n",
    "ratings     = ratings[ratings['UserID'].isin(good_users)]\n",
    "print(f\"   ✓ Ratings remaining after user filter: {len(ratings):,} rows\")\n",
    "\n",
    "# Merge ratings with wines metadata on 'WineID'\n",
    "print(\"▶ Merging ratings with wines metadata…\")\n",
    "df = pd.merge(ratings, wines, on='WineID', how='inner')\n",
    "print(f\"   ✓ Merged DataFrame: {len(df):,} rows × {df.shape[1]:,} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split of the dataset into train/val/test sets\n",
    "# (80% train+val, 20% test)\n",
    "print(\"▶ Splitting off test set (80/20 by user-time)…\")\n",
    "train_val, test = user_time_split(df, date_col='Date', split_ratio=0.8)\n",
    "print(f\"   ✓ train_val: {len(train_val):,} rows, test: {len(test):,} rows\")\n",
    "# (75% train, 25% val)\n",
    "print(\"▶ Splitting train/val (75/25 by user-time)…\")\n",
    "train, val = user_time_split(train_val, date_col='Date', split_ratio=0.75)\n",
    "print(f\"   ✓ train: {len(train):,} rows, val: {len(val):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab02176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features (X) and target (y) for each set\n",
    "# Save them as CSVs for later use\n",
    "print(\"▶ Separating features (X) and target (y)…\")\n",
    "print(\"▶ Writing out CSVs…\")\n",
    "for name, split in [('train', train), ('val', val), ('test', test)]:\n",
    "    X = split.drop(columns='Rating')\n",
    "    y = split['Rating']\n",
    "    X.to_csv(f'./data/X_{name}.csv', index=False)\n",
    "    y.to_csv(f'./data/y_{name}.csv', index=False)\n",
    "    print(f\"   ✓ Wrote {name}: X_{name}.csv ({len(X):,} rows), y_{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036273cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create profile report for the whole dataset, to get some insights on our data\n",
    "profile = ProfileReport(df, title='XWines-merged Profiling Report')\n",
    "profile.to_file('report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12386489",
   "metadata": {},
   "source": [
    "## Now merged, formatted and splitted data can be loaded from the corresponding csv files\n",
    "\n",
    "### Next steps:\n",
    "* Define the preprocessing methods for different features:\n",
    "    * **Standard Scaler** - is used for numerical type columns\n",
    "        * **ABV**\n",
    "        * **Vintage** (formatted to be numerical)\n",
    "        * **DaysAgo(Date)** - see below\n",
    "    * **One-hot-encoding** - is used for categorical features, but is limited by the number of categories within a feature:\n",
    "        * **Type**\n",
    "        * **Body**\n",
    "        * **Acidity**\n",
    "        * **Elaborate**\n",
    "    * **Multi-label Binarizer** - is used for categorical features with too many categories, where also multiple active categories could be possible:\n",
    "        * **Grapes** (774 classes)\n",
    "        * **Harmonize** (~64 classes)\n",
    "    * **Target Encoding** - used for text features and user IDs, wine IDs. Used with KFold(5 folds) to prevent data leakage, i.e. encoded feature never knows about it's own target value\n",
    "    * **Date encoding** - created custom object to convert datetime column to DaysAgo from the most recent record column. This way we keep information about time-related information and reduce feature to be simply numerical. **Standard Scaler** applied afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate features\n",
    "\n",
    "# Use StandardScaler for numerical features (create binary Is_NonVintage column derived from Vintage and maybe fill NaN values with 0)\n",
    "numerical_features = ['ABV', 'Vintage']\n",
    "# Use one-hot encoding for categorical features\n",
    "categorical_features = ['Type', 'Elaborate', 'Body', 'Acidity']\n",
    "# Use multilabel binarizer for multilabel features\n",
    "multilabel_features = ['Grapes', 'Harmonize']\n",
    "# Use target encoding for names and IDs\n",
    "text_features = ['WineryName', 'RegionName', 'Country', 'UserID', 'WineID']\n",
    "# Use conversion to DaysAgo for time-based features\n",
    "date_features = ['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ad16f",
   "metadata": {},
   "source": [
    "* **Create Preprocessing pipeline**:\n",
    "    ##### **Important**: Since during Grapes column encoding we create 774 classes + there are around 100 classes from other encoders, the pandas DataFrame would require too much RAM (I recieved 69GB memory allocation error only for the Grapes column) and same happening for the dense array (numpy), I used csr_matrix from scipy.sparse and some additional optimizations for MultiLabelBinarizer in particular, to be able to store all the preprocessed features. More info in code below.\n",
    "\n",
    "    * **Created custom object for Date column preprocessing**\n",
    "    * **Created Wrappers for other preprocessors to always return sparse csr matricies**. For OneHotEncoding there is already implemented sparse output. For StandardScaler in date_pipeline wrapper is not required, since the input is already a csr matrix.\n",
    "    * **Created pipelines for each preprocessor and a general pipeline to combine everything together, using ColumnTransformer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date preprocessor\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transforms a single datetime column into 'days ago' relative to the latest date in training data.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Expect a DataFrame with a single datetime column\n",
    "        self.col = X.columns[0]\n",
    "        self.column = f\"{self.col}_days_ago\"\n",
    "        self.reference_date = pd.to_datetime(X[self.col]).max()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        days_ago = (self.reference_date - pd.to_datetime(X[self.col])).dt.days\n",
    "        \n",
    "        return csr_matrix(days_ago.values.reshape(-1, 1))\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array([self.column])\n",
    "    \n",
    "class MultiLabelWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = []\n",
    "        for col in X.columns:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit(X[col])\n",
    "            self.encoders[col] = mlb\n",
    "            # Store feature names for this column\n",
    "            self.feature_names.extend([f\"{col}__{cls}\" for cls in mlb.classes_])\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        matricies = []\n",
    "        for col in X.columns:\n",
    "            mlb = self.encoders[col]\n",
    "            class_index = {cls: i for i, cls in enumerate(mlb.classes_)}\n",
    "            n_rows = len(X)\n",
    "            n_classes = len(mlb.classes_)\n",
    "            sparse = lil_matrix((n_rows, n_classes), dtype=np.uint8)\n",
    "\n",
    "            for i, labels in enumerate(X[col]):\n",
    "                for label in labels:\n",
    "                    if label in class_index:\n",
    "                        sparse[i, class_index[label]] = 1\n",
    "\n",
    "            matricies.append(sparse.tocsr())\n",
    "        return hstack(matricies, format='csr')\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "class TargetEncoderWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.feature_names = []\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for col in X.columns:\n",
    "            te = TargetEncoder(cols=[col])\n",
    "            te.fit(X[[col]], y, cv=kf)\n",
    "            self.encoders[col] = te\n",
    "            # Store feature names for this column\n",
    "            self.feature_names.append(f'{col}_target_encoded')\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        matricies = []\n",
    "        for col in X.columns:\n",
    "            te = self.encoders[col]\n",
    "            df_encoded = te.transform(X[[col]])\n",
    "            arr = df_encoded[col].values.reshape(-1, 1)\n",
    "            matricies.append(csr_matrix(arr))\n",
    "        return hstack(matricies, format='csr')\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "    \n",
    "class StandardScalerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler(with_mean=False)\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        # Always return sparse csr_matrix\n",
    "        if not issparse(X_scaled):\n",
    "            X_scaled = csr_matrix(X_scaled)\n",
    "        return X_scaled\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "## Preprocessing pipeline\n",
    "\n",
    "# Numerical\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('scaler', StandardScalerWrapper()),\n",
    "    ])\n",
    "\n",
    "# Categorical via one-hot-encoding\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "# Categorical via multilabelbinarizer\n",
    "multilabel_pipeline = Pipeline([\n",
    "    ('multilabel', MultiLabelWrapper())\n",
    "])\n",
    "\n",
    "# Names and IDs via target encoding\n",
    "text_pipeline = Pipeline([\n",
    "    ('target', TargetEncoderWrapper())\n",
    "])\n",
    "\n",
    "# Datetime via custom date transformer\n",
    "date_pipeline = Pipeline([\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numerical', numerical_pipeline, numerical_features),\n",
    "    ('categorical', categorical_pipeline, categorical_features),\n",
    "    ('multilabel', multilabel_pipeline, multilabel_features),\n",
    "    ('name_id', text_pipeline, text_features),\n",
    "    ('date', date_pipeline, date_features)\n",
    "])\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c89777",
   "metadata": {},
   "source": [
    "* **Fit preprocessing pipeline on training data. We pass target variable there as well for the Target Encoder**\n",
    "* **Transform X_train,X_val,_X_test on a fitted preprocessor**\n",
    "* **Save results to .npz files. These are binary format files, which allow us to store our large sparse matrices. Implemented via save_npz() for scipy.sparse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"▶ Fitting preprocessing pipeline on training data…\")\n",
    "preprocessing_pipeline.fit(X_train, y_train)\n",
    "print(\"   ✓ Pipeline fitted\")\n",
    "\n",
    "print(\"▶ Transforming X_train…\")\n",
    "X_train_preprocessed = preprocessing_pipeline.transform(X_train)\n",
    "print(f\"   ✓ X_train preprocessed: {X_train_preprocessed.shape[0]:,} rows × {X_train_preprocessed.shape[1]:,} features\")\n",
    "\n",
    "print(\"▶ Transforming X_val…\")\n",
    "X_val_preprocessed = preprocessing_pipeline.transform(X_val)\n",
    "print(f\"   ✓ X_val preprocessed: {X_val_preprocessed.shape[0]:,} rows × {X_val_preprocessed.shape[1]:,} features\")\n",
    "\n",
    "print(\"▶ Transforming X_test…\")\n",
    "X_test_preprocessed = preprocessing_pipeline.transform(X_test)\n",
    "print(f\"   ✓ X_test preprocessed: {X_test_preprocessed.shape[0]:,} rows × {X_test_preprocessed.shape[1]:,} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to NPZ files\n",
    "print(\"▶ Writing out preprocessed data to NPZs…\")\n",
    "save_npz(\"X_train_preprocessed.npz\", X_train_preprocessed)\n",
    "print(f\"   ✓ Wrote X_train_processed.npz ({X_train_preprocessed.shape[0]:,} rows × {X_train_preprocessed.shape[1]:,} features)\")\n",
    "save_npz(\"X_val_preprocessed.npz\", X_val_preprocessed)\n",
    "print(f\"   ✓ Wrote X_val_processed.npz ({X_val_preprocessed.shape[0]:,} rows × {X_val_preprocessed.shape[1]:,} features)\")\n",
    "save_npz(\"X_test_preprocessed.npz\", X_test_preprocessed)\n",
    "print(f\"   ✓ Wrote X_test_processed.npz ({X_test_preprocessed.shape[0]:,} rows × {X_test_preprocessed.shape[1]:,} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171e3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13c4a9fd",
   "metadata": {},
   "source": [
    "* **Few test runs for different models without Hyperparameter optimization to confirm everything works.**\n",
    "* **Do Hyperparameter optimization. Unfortunately we can't use Cross-Validation since our dataset is too large. After some research I decided to try optuna package which is quite easy to use. However, will need to see if it gives reasonable result in our case.**\n",
    "\n",
    "#### Below is only sample for LightGBM model. I will add and run other models over the weekends and then we could benchmark the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad505ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred = lgb_model.predict(X_test_preprocessed)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "\n",
    "# I lost output for this cell, but it was around MSE: 0.37. Will rerun all cells again soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be097fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    model = lgb.LGBMRegressor(\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        num_leaves=trial.suggest_int('num_leaves', 31, 256),\n",
    "        max_depth=trial.suggest_int('max_depth', -1, 20),\n",
    "        min_child_samples=trial.suggest_int('min_child_samples', 5, 100),\n",
    "        n_estimators=1000\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_preprocessed, y_train,\n",
    "        eval_set=[(X_val_preprocessed, y_val)],\n",
    "        eval_metric='mse',\n",
    "        callbacks=[lgb.early_stopping(50)],\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_val_preprocessed)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    return mse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "print(\"✅ Best params:\", study.best_params)\n",
    "print(\"✅ Best MSE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.0503092382442247, num_leaves=169, max_depth=8, min_child_samples=82, random_state=42)\n",
    "lgb_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred = lgb_model.predict(X_test_preprocessed)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "\n",
    "# Result MSE: 0.3955, falls in a \"good\" range for target value in range 1 to 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
