{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7f6537",
   "metadata": {},
   "source": [
    "# Content-based recommender\n",
    "## Using: cosine similarity\n",
    "## Predicting: Ratings, Top-K recommendations\n",
    "## Next steps: try autoencoders for content-based recommendations or go straight to GNNs?\n",
    "\n",
    "### Workflow:\n",
    "* **Read the data**. During this step I keep only required columns, format dates for the time-awared train/test split, format string lists(columns: Grapes, Harmonize). \n",
    "* **Train/test split**. 80/20 split based on dates, to keep more recent records in test split.\n",
    "* **Preprocess wines data**:\n",
    "    * **StandardScaler** - ABV numeric column\n",
    "    * **One-hot-encoding** - Type, ELaborate, Body, Acidity, since these columns have reasonable number of classes.\n",
    "    * **Frequency encoding** - WineryName, RegionName, Country\n",
    "    * **TF-IDF or MultiLabelBinarizer** - Grapes, Harmonize. These columns could be treated both as categories or text features. I will compare the differense in results for both types of encoding.\n",
    "    Note: For preprocessing custom Wrappers were created to make compatible in the pipeline and output csr_matrix in all transformers. \n",
    "    * **Create user profiles**. Normalize both user profiles and wine matrix.\n",
    "    * **Predict ratings**. Evaluate using MAE and RMSE.\n",
    "    * **Give top-k recommendations** Evaluate using Recall@k. I will add more metrics later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5c4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer, normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "from scipy.sparse import csr_matrix, issparse, lil_matrix, hstack, vstack, save_npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2244d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converter of string lists into Python lists\n",
    "# (e.g. \"['a', 'b', 'c']\" → [a, b, c])\n",
    "def parse_list_col(s):\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Format strings lists into a space-separated string\n",
    "# (e.g. \"['a', 'b', 'c']\" → \"a b c\")\n",
    "def parse_list_col_str(s):\n",
    "    return s.strip(\"[]\").replace(\"'\", \"\").replace(',', ' ')\n",
    "\n",
    "# Time-based split of a DataFrame by user\n",
    "# (e.g. 80% train, 20% test) based on the date column\n",
    "def user_time_split(df, date_col='Date', split_ratio=0.8):\n",
    "    train_parts, val_parts = [], []\n",
    "    for uid, group in df.groupby('UserID', sort=False):\n",
    "        group = group.sort_values(date_col)\n",
    "        i = int(len(group) * split_ratio)\n",
    "        train_parts.append(group.iloc[:i])\n",
    "        val_parts.append(group.iloc[i:])\n",
    "    return pd.concat(train_parts), pd.concat(val_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3feef2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Reading wines metadata…\n",
      "   ✓ Loaded wines: 100,646 rows\n",
      "▶ Reading ratings…\n",
      "   ✓ Loaded ratings: 21,013,536 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the wines data and parse string lists into Python lists\n",
    "print(\"▶ Reading wines metadata…\")\n",
    "wines = pd.read_csv(\n",
    "    './data/XWines_Full_100K_wines.csv',\n",
    "    usecols=['WineID', 'Type', 'Elaborate', 'ABV', 'Body', 'Acidity', 'RegionName', 'WineryName', 'Grapes','Harmonize','Country'],\n",
    "    converters={\n",
    "        'Grapes':    parse_list_col_str,\n",
    "        'Harmonize': parse_list_col_str\n",
    "    }\n",
    ")\n",
    "print(f\"   ✓ Loaded wines: {len(wines):,} rows\")\n",
    "\n",
    "# Save the wine IDs for later use\n",
    "wines = wines.set_index('WineID')\n",
    "wine_ids = wines.index.tolist()\n",
    "\n",
    "# Read the ratings data and parse 'N.V.' into 0 (Vitage to numeric), parse dates\n",
    "print(\"▶ Reading ratings…\")\n",
    "ratings = pd.read_csv(\n",
    "    './data/XWines_Full_21M_ratings.csv',\n",
    "    usecols=['UserID','WineID','Date','Rating'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    ")\n",
    "print(f\"   ✓ Loaded ratings: {len(ratings):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad5a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Filtering wines with ≥5 ratings…\n",
      "   ✓ Ratings remaining after wine filter: 21,013,536 rows\n",
      "▶ Filtering users with ≥5 reviews…\n",
      "   ✓ Ratings remaining after user filter: 21,013,536 rows\n"
     ]
    }
   ],
   "source": [
    "## Noise reduction\n",
    "\n",
    "# Filter out wines with fewer than 5 ratings \n",
    "print(\"▶ Filtering wines with ≥5 ratings…\")\n",
    "wine_counts = ratings['WineID'].value_counts()\n",
    "good_wines  = wine_counts[wine_counts >= 5].index\n",
    "ratings     = ratings[ratings['WineID'].isin(good_wines)]\n",
    "print(f\"   ✓ Ratings remaining after wine filter: {len(ratings):,} rows\")\n",
    "\n",
    "# Filter out users with fewer than 5 reviews\n",
    "print(\"▶ Filtering users with ≥5 reviews…\")\n",
    "user_counts = ratings['UserID'].value_counts()\n",
    "good_users  = user_counts[user_counts >= 5].index\n",
    "ratings     = ratings[ratings['UserID'].isin(good_users)]\n",
    "print(f\"   ✓ Ratings remaining after user filter: {len(ratings):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a39d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Splitting off test set (80/20 by user-time)…\n",
      "   ✓ train: 16,396,768 rows, test: 4,616,768 rows\n"
     ]
    }
   ],
   "source": [
    "# Time-based split of the ratings dataset into train/val/test sets\n",
    "# (80% train+val, 20% test)\n",
    "print(\"▶ Splitting off test set (80/20 by user-time)…\")\n",
    "train, test = user_time_split(ratings, date_col='Date', split_ratio=0.8)\n",
    "print(f\"   ✓ train: {len(train):,} rows, test: {len(test):,} rows\")\n",
    "# # (75% train, 25% val)\n",
    "# print(\"▶ Splitting train/val (75/25 by user-time)…\")\n",
    "# train, val = user_time_split(train_val, date_col='Date', split_ratio=0.75)\n",
    "# print(f\"   ✓ train: {len(train):,} rows, val: {len(val):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb63756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate features\n",
    "\n",
    "# Use StandardScaler for numerical features\n",
    "numerical_features = ['ABV']\n",
    "# Use one-hot encoding for categorical features\n",
    "categorical_features = ['Type', 'Elaborate', 'Body', 'Acidity']\n",
    "# Use TF-IDF for text features\n",
    "text_features = ['Grapes', 'Harmonize']\n",
    "# Use frequency encoding for categorical features\n",
    "freq_features = ['WineryName', 'RegionName', 'Country']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f482eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScalerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler(with_mean=False)\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        # Always return sparse csr_matrix\n",
    "        if not issparse(X_scaled):\n",
    "            X_scaled = csr_matrix(X_scaled)\n",
    "        return X_scaled\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "class TFIDFWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        texts = X.apply(lambda row: ' '.join(row.values), axis=1)\n",
    "        self.tfidf.fit(texts)\n",
    "        self.feature_names = self.tfidf.get_feature_names_out()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        texts = X.apply(lambda row: ' '.join(row.values), axis=1)\n",
    "        X_tfidf = self.tfidf.transform(texts)\n",
    "        return csr_matrix(X_tfidf)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "class MultiLabelWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = []\n",
    "        for col in X.columns:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            mlb.fit(X[col])\n",
    "            self.encoders[col] = mlb\n",
    "            # Store feature names for this column\n",
    "            self.feature_names.extend([f\"{col}__{cls}\" for cls in mlb.classes_])\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        matricies = []\n",
    "        for col in X.columns:\n",
    "            mlb = self.encoders[col]\n",
    "            class_index = {cls: i for i, cls in enumerate(mlb.classes_)}\n",
    "            n_rows = len(X)\n",
    "            n_classes = len(mlb.classes_)\n",
    "            sparse = lil_matrix((n_rows, n_classes), dtype=np.uint8)\n",
    "\n",
    "            for i, labels in enumerate(X[col]):\n",
    "                for label in labels:\n",
    "                    if label in class_index:\n",
    "                        sparse[i, class_index[label]] = 1\n",
    "\n",
    "            matricies.append(sparse.tocsr())\n",
    "        return hstack(matricies, format='csr')\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "class FrequencyEncoder:\n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        for col in X.columns:\n",
    "            freq = X[col].value_counts(normalize=True)\n",
    "            self.freq_maps[col] = freq\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        matrices = []\n",
    "        for col in X.columns:\n",
    "            freq_map = self.freq_maps.get(col, {})\n",
    "            col_freq = X[col].map(freq_map).fillna(0).values.reshape(-1, 1)\n",
    "            col_freq = csr_matrix(col_freq)\n",
    "            matrices.append(col_freq)\n",
    "        return hstack(matrices, format='csr')\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array([f\"{col}_freq\" for col in self.feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa4525bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing pipeline\n",
    "\n",
    "# Numerical\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('scaler', StandardScalerWrapper()),\n",
    "    ])\n",
    "\n",
    "# Categorical via one-hot-encoding\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "# Text features via TF-IDF\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('tfidf', TFIDFWrapper())\n",
    "])\n",
    "\n",
    "# Categorical features via frequency encoding\n",
    "freq_pipeline = Pipeline([\n",
    "    ('date', FrequencyEncoder()),\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numerical', numerical_pipeline, numerical_features),\n",
    "    ('categorical', categorical_pipeline, categorical_features),\n",
    "    ('tfidf', tfidf_pipeline, text_features),\n",
    "    ('names', freq_pipeline, freq_features),\n",
    "\n",
    "])\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49cc2142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\denis\\Envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\denis\\Envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the wines data\n",
    "preprocessing_pipeline.fit(wines)\n",
    "wines_features = preprocessing_pipeline.transform(wines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35030ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Match WineID to the index of the wines_features matrix\n",
    "wine_id_to_idx = {wine_id: idx for idx, wine_id in enumerate(wine_ids)}\n",
    "\n",
    "# Get unique userIDs and assign them to indices\n",
    "user_ids = train['UserID'].unique()\n",
    "user_id_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5bbd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Ratings matrix shape: (1056079, 100646)\n"
     ]
    }
   ],
   "source": [
    "# Create a sparse matrix for the ratings train data\n",
    "\n",
    "# Map user IDs and wine IDs to indices\n",
    "row_idx = train['UserID'].map(user_id_to_idx)\n",
    "col_idx = train['WineID'].map(wine_id_to_idx)\n",
    "# Make sure no NaNs\n",
    "mask = row_idx.notna() & col_idx.notna()\n",
    "row_idx = row_idx[mask].astype(int)\n",
    "col_idx = col_idx[mask].astype(int)\n",
    "# Get the ratings values\n",
    "data = train.loc[mask, 'Rating']\n",
    "\n",
    "# Create the sparse matrix\n",
    "ratings_train = csr_matrix((data, (row_idx, col_idx)), \n",
    "                     shape=(len(user_id_to_idx), len(wine_id_to_idx)))\n",
    "print(f\"   ✓ Ratings matrix shape: {ratings_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e685efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize features for cosine similarity\n",
    "wines_norm = normalize(wines_features, axis=1)\n",
    "\n",
    "# Create user profile matrix\n",
    "user_profiles = normalize(ratings_train.dot(wines_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_test_pairs(user_profiles, wines, ratings_test, user_id_to_idx, item_id_to_idx):\n",
    "    preds = []\n",
    "\n",
    "    for _, row in ratings_test.iterrows():\n",
    "        u_id, i_id = row[\"UserID\"], row[\"WineID\"]\n",
    "        if u_id not in user_id_to_idx or i_id not in item_id_to_idx:\n",
    "            continue \n",
    "        u_idx = user_id_to_idx[u_id]\n",
    "        i_idx = item_id_to_idx[i_id]\n",
    "\n",
    "        score = user_profiles[u_idx].dot(wines[i_idx].T).toarray().item()\n",
    "        pred_rating = 1 + 4 * ((score + 1) / 2)  # rescale\n",
    "        preds.append(pred_rating)\n",
    "    return preds\n",
    "\n",
    "# def predict_ratings_batch(user_profiles, wines_norm, batch_size=1000):\n",
    "#     n_users = user_profiles.shape[0]\n",
    "#     n_wines = wines_norm.shape[0]\n",
    "#     predicted_rows =[]\n",
    "\n",
    "#     for start in range(0, n_users, batch_size):\n",
    "#         end = min(start + batch_size, n_users)\n",
    "#         batch_profiles = user_profiles[start:end]\n",
    "#         # Compute the predicted ratings for the current batch\n",
    "#         print(f\"Scoring users {start} to {end - 1}, batch size: {batch_profiles.shape[0]}\")\n",
    "#         pred_ratings_batch = batch_profiles.dot(wines_norm.T).toarray()\n",
    "#         # Scale to 1-5 range\n",
    "#         pred_ratings_batch = 1 + 4 * ((pred_ratings_batch + 1) / 2)\n",
    "#         predicted_rows.append(pred_ratings_batch)\n",
    "\n",
    "#     return np.vstack(predicted_rows)\n",
    "\n",
    "# def get_predicted_rating(user_id, wine_id, user_id_to_idx, wine_id_to_idx, ratings_pred):\n",
    "#     u = user_id_to_idx.get(user_id)\n",
    "#     i = wine_id_to_idx.get(wine_id)\n",
    "#     if u is not None and i is not None:\n",
    "#         print(f\"Predicted rating for user {user_id} and wine {wine_id}: {ratings_pred.shape}\")\n",
    "#         return ratings_pred[u, i]\n",
    "#     else:\n",
    "#         return np.nan  # for unknown user/item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_pred = predict_ratings_batch(user_profiles, wines_norm)\n",
    "ratings_pred = predict_for_test_pairs(user_profiles, wines_norm, test, user_id_to_idx, wine_id_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4aff0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RMSE: 1.1927\n",
      "✅ MAE:  0.9830\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test['Rating_pred'] = test.apply(\n",
    "#     lambda row: get_predicted_rating(row['UserID'], row['WineID'], \n",
    "#                                      user_id_to_idx, wine_id_to_idx, ratings_pred), axis=1)\n",
    "\n",
    "# Drop unknown predictions (e.g., cold-start)\n",
    "# test = test.dropna(subset=['Rating_pred'])\n",
    "\n",
    "rmse = root_mean_squared_error(test['Rating'], ratings_pred)\n",
    "mae = mean_absolute_error(test['Rating'], ratings_pred)\n",
    "\n",
    "print(f\"✅ RMSE: {rmse:.4f}\")\n",
    "print(f\"✅ MAE:  {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2779821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recommend_top_k_batch(user_profiles, wines_norm, ratings_train, k=5, batch_size=1000):\n",
    "    recommendations = []\n",
    "    n_users = user_profiles.shape[0]\n",
    "    print(f\"   ✓ Number of users: {n_users}\")\n",
    "    for start in range(0, n_users, batch_size):\n",
    "        end = min(start + batch_size, n_users)\n",
    "        batch_profiles = user_profiles[start:end]\n",
    "\n",
    "        # Compute cosine similarities for this batch\n",
    "        print(f\"Scoring users {start} to {end - 1}, batch size: {batch_profiles.shape[0]}\")\n",
    "        scores = batch_profiles.dot(wines_norm.T)  # shape: (batch_size, n_items)\n",
    "\n",
    "        for i, u in enumerate(range(start, end)):\n",
    "            # Get the user's profile and the scores for all items\n",
    "            user_row = ratings_train.getrow(u)\n",
    "            user_seen = user_row.indices\n",
    "            score_row = scores[i].toarray().ravel()\n",
    "            # Mask out items the user has already rated\n",
    "            score_row[user_seen] = -np.inf\n",
    "            valid_idx = np.where(score_row > -np.inf)[0]\n",
    "\n",
    "            # Sort the scores and get the top k items\n",
    "            # If the user has rated all items, return an empty list\n",
    "            if len(valid_idx) == 0:\n",
    "                top_k_sorted = []\n",
    "            else:\n",
    "                k_actual = min(k, len(valid_idx))\n",
    "                top_k = np.argpartition(-score_row, k_actual)[:k_actual]\n",
    "                top_k_sorted = top_k[np.argsort(-score_row[top_k])]\n",
    "            recommendations.append(top_k_sorted)\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572cfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Number of users: 1056079\n",
      "Scoring users 0 to 9999, batch size: 10000\n",
      "Scoring users 10000 to 19999, batch size: 10000\n",
      "Scoring users 20000 to 29999, batch size: 10000\n",
      "Scoring users 30000 to 39999, batch size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Get top-5 recommendations for each user\n",
    "top_k = 5\n",
    "recommendations = recommend_top_k_batch(user_profiles, wines_norm, ratings_train, top_k, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb7f5b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5: 0.0166\n"
     ]
    }
   ],
   "source": [
    "def recall_at_k(recommendations, test_df, user_id_to_idx, item_id_to_idx, k=10):\n",
    "    hit_count = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Group test data by user\n",
    "    test_groups = test_df.groupby('UserID')['WineID'].apply(set).to_dict()\n",
    "\n",
    "    for user_id, relevant_items in test_groups.items():\n",
    "        if user_id not in user_id_to_idx:\n",
    "            continue  # skip unknown users\n",
    "        u_idx = user_id_to_idx[user_id]\n",
    "        recs = recommendations[u_idx][:k]\n",
    "        rec_ids = set([idx for idx in recs if idx in item_id_to_idx.values()])\n",
    "\n",
    "        hit_count += len(rec_ids.intersection({item_id_to_idx[i] for i in relevant_items if i in item_id_to_idx}))\n",
    "        total += len(relevant_items)\n",
    "\n",
    "    return hit_count / total if total > 0 else 0\n",
    "\n",
    "\n",
    "# Calculate recall@5\n",
    "recall = recall_at_k(recommendations, test, user_id_to_idx, wine_id_to_idx, k=top_k)\n",
    "print(f\"Recall@{top_k}: {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
