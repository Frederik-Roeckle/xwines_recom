{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4813d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_topk_fast(df, k=10):\n",
    "    # Pre-sort so top-k is at the top per user\n",
    "    df = df.sort_values(['UserID', 'Rank_pred'], ascending=[True, True])\n",
    "\n",
    "    # Assign group index per row (unique integer per user)\n",
    "    user_index, user_pos = np.unique(df['UserID'], return_inverse=True)\n",
    "\n",
    "    # Count items per user\n",
    "    user_counts = np.bincount(user_pos)\n",
    "    user_offsets = np.zeros(len(df), dtype=int)\n",
    "    np.add.at(user_offsets, np.cumsum(user_counts)[:-1], 1)\n",
    "    user_offsets = np.cumsum(user_offsets)\n",
    "\n",
    "    # Mask to keep only top-k per user\n",
    "    df['row_number'] = df.groupby('UserID').cumcount()\n",
    "    topk_df = df[df['row_number'] < k].copy()\n",
    "\n",
    "    # Precision@k\n",
    "    precision = topk_df['Relevance'].groupby(topk_df['UserID']).mean().mean()\n",
    "\n",
    "    # Recall@k\n",
    "    relevant_per_user = df.groupby('UserID')['Relevance'].sum()\n",
    "    hits_per_user = topk_df.groupby('UserID')['Relevance'].sum()\n",
    "    recall = (hits_per_user / relevant_per_user).fillna(0).mean()\n",
    "\n",
    "    # HitRate@k\n",
    "    hits = (hits_per_user > 0).astype(int)\n",
    "    hit_rate = hits.mean()\n",
    "\n",
    "    # MAP@k\n",
    "    def map_at_k_per_user(x):\n",
    "        rels = x['Relevance'].values\n",
    "        precisions = [(rels[:i + 1].sum() / (i + 1)) for i in range(len(rels)) if rels[i]]\n",
    "        return np.mean(precisions) if precisions else 0\n",
    "    mapk = topk_df.groupby('UserID').apply(map_at_k_per_user).mean()\n",
    "\n",
    "    # nDCG@k\n",
    "    def dcg(rels):\n",
    "        return np.sum(rels / np.log2(np.arange(2, len(rels) + 2)))\n",
    "    def ndcg_per_user(x):\n",
    "        dcg_val = dcg(x['Relevance'].values)\n",
    "        ideal = x.sort_values('Relevance', ascending=False).head(k)\n",
    "        idcg_val = dcg(ideal['Relevance'].values)\n",
    "        return dcg_val / idcg_val if idcg_val > 0 else 0\n",
    "    ndcg = topk_df.groupby('UserID').apply(ndcg_per_user).mean()\n",
    "\n",
    "    return {\n",
    "        'Precision@k': precision,\n",
    "        'Recall@k': recall,\n",
    "        'HitRate@k': hit_rate,\n",
    "        'MAP@k': mapk,\n",
    "        'nDCG@k': ndcg\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84091bf5",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300345ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_uwarm_iwarm = pd.read_csv(f'{base_path}\\\\lightgbm\\\\lightgbm_warm_user_warm_item.csv', usecols=['RatingID', 'Rating'], header= 0, names=['RatingID','Rating_pred'])\n",
    "results_uwarm_icold = pd.read_csv(f'{base_path}\\\\lightgbm\\\\lightgbm_warm_user_cold_item.csv', usecols=['RatingID', 'Rating'], header= 0, names=['RatingID','Rating_pred'])\n",
    "results_ucold_iwarm = pd.read_csv(f'{base_path}\\\\lightgbm\\\\lightgbm_cold_user_warm_item.csv', usecols=['RatingID', 'Rating'], header= 0, names=['RatingID','Rating_pred'])\n",
    "results_ucold_icold = pd.read_csv(f'{base_path}\\\\lightgbm\\\\lightgbm_cold_user_cold_item.csv', usecols=['RatingID', 'Rating'], header= 0, names=['RatingID','Rating_pred'])\n",
    "\n",
    "# Load the test set\n",
    "test_uwarm_iwarm = pd.read_csv(f'{base_path}\\\\testset_warm_user_warm_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_uwarm_icold = pd.read_csv(f'{base_path}\\\\testset_warm_user_cold_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_ucold_iwarm = pd.read_csv(f'{base_path}\\\\testset_cold_user_warm_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_ucold_icold = pd.read_csv(f'{base_path}\\\\testset_cold_user_cold_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "\n",
    "# Merge the results with the test set\n",
    "results_uwarm_iwarm = results_uwarm_iwarm.merge(test_uwarm_iwarm, on='RatingID', how='left')\n",
    "results_uwarm_icold = results_uwarm_icold.merge(test_uwarm_icold, on='RatingID', how='left')\n",
    "results_ucold_iwarm = results_ucold_iwarm.merge(test_ucold_iwarm, on='RatingID', how='left')\n",
    "results_ucold_icold = results_ucold_icold.merge(test_ucold_icold, on='RatingID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9415e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mse_uwarm_iwarm = mean_squared_error(results_uwarm_iwarm['Rating'], results_uwarm_iwarm['Rating_pred'])\n",
    "mse_uwarm_icold = mean_squared_error(y_test_uwarm_icold, results_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(y_test_ucold_iwarm, results_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(y_test_ucold_icold, results_ucold_icold)\n",
    "# RMSE\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(y_test_uwarm_iwarm, results_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(y_test_uwarm_icold, results_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(y_test_ucold_iwarm, results_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(y_test_ucold_icold, results_ucold_icold)\n",
    "# MAE\n",
    "mae_uwarm_iwarm = mean_absolute_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(y_test_ucold_icold, y_pred_ucold_icold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Rank and Rank_pred columns\n",
    "\n",
    "# Warm user, warm item\n",
    "results_uwarm_iwarm[\"Rank\"] = results_uwarm_iwarm.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_uwarm_iwarm[\"Rank_pred\"] = results_uwarm_iwarm.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Warm user, cold item\n",
    "results_uwarm_icold[\"Rank\"] = results_uwarm_icold.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_uwarm_icold[\"Rank_pred\"] = results_uwarm_icold.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Cold user, warm item\n",
    "results_ucold_iwarm[\"Rank\"] = results_ucold_iwarm.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_ucold_iwarm[\"Rank_pred\"] = results_ucold_iwarm.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Cold user, cold item\n",
    "results_ucold_icold[\"Rank\"] = results_ucold_icold.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_ucold_icold[\"Rank_pred\"] = results_ucold_icold.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "# Calculate Relevance\n",
    "results_uwarm_iwarm[\"Relevance\"] = results_uwarm_iwarm[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_uwarm_icold[\"Relevance\"] = results_uwarm_icold[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_ucold_iwarm[\"Relevance\"] = results_ucold_iwarm[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_ucold_icold[\"Relevance\"] = results_ucold_icold[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddd4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "k = 10\n",
    "results = {}\n",
    "results['warm user, warm item'] = evaluate_topk_fast(results_uwarm_iwarm, k=k)\n",
    "results['warm user, cold item'] = evaluate_topk_fast(results_uwarm_icold, k=k)\n",
    "results['cold user, warm item'] = evaluate_topk_fast(results_ucold_iwarm, k=k)\n",
    "results['cold user, cold item'] = evaluate_topk_fast(results_ucold_icold, k=k)\n",
    "\n",
    "# Print evaluation results\n",
    "for case, metrics in results.items():\n",
    "    print(f\"Evaluation on {case} at top {k}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099cde94",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0964ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_uwarm_iwarm = pd.read_csv(f'{base_path}\\\\xgboost\\\\xgboost_warm_user_warm_item.csv', usecols=['RatingID', 'Rating'], header= 0, names=['RatingID','Rating_pred'])\n",
    "results_uwarm_icold = pd.read_csv(f'{base_path}\\\\xgboost\\\\xgboost_warm_user_cold_item.csv', usecols=['RatingID', 'Rating'], header= 0, names=['RatingID','Rating_pred'])\n",
    "results_ucold_iwarm = pd.read_csv(f'{base_path}\\\\xgboost\\\\xgboost_cold_user_warm_item.csv', usecols=['RatingID', 'Rating'], header= 0, names=['RatingID','Rating_pred'])\n",
    "results_ucold_icold = pd.read_csv(f'{base_path}\\\\xgboost\\\\xgboost_cold_user_cold_item.csv', usecols=['RatingID', 'Rating'], header= 0, names=['RatingID','Rating_pred'])\n",
    "\n",
    "# Load the test set\n",
    "test_uwarm_iwarm = pd.read_csv(f'{base_path}\\\\testset_warm_user_warm_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_uwarm_icold = pd.read_csv(f'{base_path}\\\\testset_warm_user_cold_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_ucold_iwarm = pd.read_csv(f'{base_path}\\\\testset_cold_user_warm_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_ucold_icold = pd.read_csv(f'{base_path}\\\\testset_cold_user_cold_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "\n",
    "# Merge the results with the test set\n",
    "results_uwarm_iwarm = results_uwarm_iwarm.merge(test_uwarm_iwarm, on='RatingID', how='left')\n",
    "results_uwarm_icold = results_uwarm_icold.merge(test_uwarm_icold, on='RatingID', how='left')\n",
    "results_ucold_iwarm = results_ucold_iwarm.merge(test_ucold_iwarm, on='RatingID', how='left')\n",
    "results_ucold_icold = results_ucold_icold.merge(test_ucold_icold, on='RatingID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mse_uwarm_iwarm = mean_squared_error(results_uwarm_iwarm['Rating'], results_uwarm_iwarm['Rating_pred'])\n",
    "mse_uwarm_icold = mean_squared_error(y_test_uwarm_icold, results_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(y_test_ucold_iwarm, results_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(y_test_ucold_icold, results_ucold_icold)\n",
    "# RMSE\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(y_test_uwarm_iwarm, results_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(y_test_uwarm_icold, results_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(y_test_ucold_iwarm, results_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(y_test_ucold_icold, results_ucold_icold)\n",
    "# MAE\n",
    "mae_uwarm_iwarm = mean_absolute_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(y_test_ucold_icold, y_pred_ucold_icold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Rank and Rank_pred columns\n",
    "\n",
    "# Warm user, warm item\n",
    "results_uwarm_iwarm[\"Rank\"] = results_uwarm_iwarm.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_uwarm_iwarm[\"Rank_pred\"] = results_uwarm_iwarm.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Warm user, cold item\n",
    "results_uwarm_icold[\"Rank\"] = results_uwarm_icold.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_uwarm_icold[\"Rank_pred\"] = results_uwarm_icold.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Cold user, warm item\n",
    "results_ucold_iwarm[\"Rank\"] = results_ucold_iwarm.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_ucold_iwarm[\"Rank_pred\"] = results_ucold_iwarm.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Cold user, cold item\n",
    "results_ucold_icold[\"Rank\"] = results_ucold_icold.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_ucold_icold[\"Rank_pred\"] = results_ucold_icold.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "# Calculate Relevance\n",
    "results_uwarm_iwarm[\"Relevance\"] = results_uwarm_iwarm[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_uwarm_icold[\"Relevance\"] = results_uwarm_icold[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_ucold_iwarm[\"Relevance\"] = results_ucold_iwarm[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_ucold_icold[\"Relevance\"] = results_ucold_icold[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7101b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "k = 10\n",
    "results = {}\n",
    "results['warm user, warm item'] = evaluate_topk_fast(results_uwarm_iwarm, k=k)\n",
    "results['warm user, cold item'] = evaluate_topk_fast(results_uwarm_icold, k=k)\n",
    "results['cold user, warm item'] = evaluate_topk_fast(results_ucold_iwarm, k=k)\n",
    "results['cold user, cold item'] = evaluate_topk_fast(results_ucold_icold, k=k)\n",
    "\n",
    "# Print evaluation results\n",
    "for case, metrics in results.items():\n",
    "    print(f\"Evaluation on {case} at top {k}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
