{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3843d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from scipy.sparse import hstack, csr_matrix, issparse, lil_matrix, save_npz, load_npz\n",
    "from sklearn.model_selection import KFold\n",
    "from tabpfn import TabPFNRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c24e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter of string lists into Python lists\n",
    "# (e.g. \"['a', 'b', 'c']\" â†’ [a, b, c])\n",
    "def parse_list_col(s):\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Converter of 'N.V.' to 0, so column is numeric\n",
    "def parse_vintage(s):\n",
    "    return 0 if s == 'N.V.' else int(s)\n",
    "\n",
    "\n",
    "base_path = '..\\..\\..\\..\\data\\main'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af17ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test splits\n",
    "\n",
    "wines = pd.read_csv(\n",
    "    f'{base_path}\\\\XWines_Full_100K_wines.csv', \n",
    "    usecols=['WineID', 'Type', 'Elaborate', 'ABV', 'Body', 'Acidity', 'RegionName', 'WineryName', 'Grapes','Harmonize','Country'],\n",
    "    converters={\n",
    "        'Grapes':    parse_list_col,\n",
    "        'Harmonize': parse_list_col\n",
    "    }\n",
    ")\n",
    "train = pd.read_csv(\n",
    "    f'{base_path}\\\\trainset.csv', \n",
    "    usecols=['UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_uwarm_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_warm_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_uwarm_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_warm_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_ucold_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_cold_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_ucold_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_cold_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210481ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop target column for all datasets\n",
    "\n",
    "X_train = train.drop(columns=['Rating'])\n",
    "y_train = train['Rating']\n",
    "\n",
    "X_test_uwarm_iwarm = test_uwarm_iwarm.drop(columns=['Rating'])\n",
    "y_test_uwarm_iwarm = test_uwarm_iwarm['Rating']\n",
    "\n",
    "X_test_uwarm_icold = test_uwarm_icold.drop(columns=['Rating'])\n",
    "y_test_uwarm_icold = test_uwarm_icold['Rating']\n",
    "\n",
    "X_test_ucold_iwarm = test_ucold_iwarm.drop(columns=['Rating'])\n",
    "y_test_ucold_iwarm = test_ucold_iwarm['Rating']\n",
    "\n",
    "X_test_ucold_icold = test_ucold_icold.drop(columns=['Rating'])\n",
    "y_test_ucold_icold = test_ucold_icold['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d551b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take a small sample of the training npz set\n",
    "X_train = X_train.sample(frac=0.1, random_state=42)\n",
    "y_train = y_train.iloc[X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d35947",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 8.70 GiB is allocated by PyTorch, and 36.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m clf = TabPFNRegressor(device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# or 'cpu', but very slow\u001b[39;00m\n\u001b[32m      8\u001b[39m clf.fit(X_train.iloc[sample_idx], y_train.iloc[sample_idx])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tabpfn_preds += \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m / \u001b[32m5\u001b[39m  \u001b[38;5;66;03m# average predictions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:81\u001b[39m, in \u001b[36mContextDecorator.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwds):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._recreate_cm():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\tabpfn\\regressor.py:649\u001b[39m, in \u001b[36mTabPFNRegressor.predict\u001b[39m\u001b[34m(self, X, output_type, quantiles)\u001b[39m\n\u001b[32m    646\u001b[39m outputs: \u001b[38;5;28mlist\u001b[39m[torch.Tensor] = []\n\u001b[32m    647\u001b[39m borders: \u001b[38;5;28mlist\u001b[39m[np.ndarray] = []\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecutor_\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muse_autocast_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRegressorEnsembleConfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoftmax_temperature\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\tabpfn\\inference.py:332\u001b[39m, in \u001b[36mInferenceEngineCachePreprocessing.iter_outputs\u001b[39m\u001b[34m(self, X, device, autocast, only_return_standard_out)\u001b[39m\n\u001b[32m    326\u001b[39m style = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    329\u001b[39m     torch.autocast(device.type, enabled=autocast),\n\u001b[32m    330\u001b[39m     torch.inference_mode(),\n\u001b[32m    331\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_return_standard_out\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_return_standard_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_inds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcat_ix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_eval_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m output = output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m output, config\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\tabpfn\\model\\transformer.py:416\u001b[39m, in \u001b[36mPerFeatureTransformer.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) == \u001b[32m3\u001b[39m:\n\u001b[32m    415\u001b[39m     style, x, y = args\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnrecognized input. Please follow the doc string.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\tabpfn\\model\\transformer.py:619\u001b[39m, in \u001b[36mPerFeatureTransformer._forward\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;66;03m# b s f e + b s 1 e -> b s f+1 e\u001b[39;00m\n\u001b[32m    617\u001b[39m embedded_input = torch.cat((embedded_x, embedded_y.unsqueeze(\u001b[32m2\u001b[39m)), dim=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_input\u001b[49m\u001b[43m)\u001b[49m.any():\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    621\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThere should be no NaNs in the encoded x and y.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    622\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck that you do not feed NaNs or use a NaN-handling enocder.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    623\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYour embedded x and y returned the following:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    624\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.isnan(embedded_x).any()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.isnan(embedded_y).any()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m embedded_y, embedded_x\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 8.70 GiB is allocated by PyTorch, and 36.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Split trainset into smaller subsets\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "tabpfn_preds = np.zeros((X_train.shape[0],))\n",
    "\n",
    "for train_idx, _ in kf.split(X_train):\n",
    "    sample_idx = np.random.choice(train_idx, size=8192, replace=False)  # sample subset\n",
    "    clf = TabPFNRegressor(device='cuda')  # or 'cpu', but very slow\n",
    "    clf.fit(X_train.iloc[sample_idx], y_train.iloc[sample_idx])\n",
    "    tabpfn_preds += clf.predict(X_train) / 5  # average predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformed data from npz\n",
    "X_train_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_train_transformed.npz')\n",
    "X_val_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_val_transformed.npz')\n",
    "X_test_uwarm_iwarm_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_test_uwarm_iwarm_transformed.npz')\n",
    "X_test_uwarm_icold_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_test_uwarm_icold_transformed.npz')\n",
    "X_test_ucold_iwarm_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_test_ucold_iwarm_transformed.npz')\n",
    "X_test_ucold_icold_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_test_ucold_icold_transformed.npz')\n",
    "\n",
    "# Load target variables\n",
    "y_train = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_train.csv')\n",
    "y_val = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_val.csv')\n",
    "y_test_uwarm_iwarm = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_test_uwarm_iwarm.csv')\n",
    "y_test_uwarm_icold = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_test_uwarm_icold.csv')\n",
    "y_test_ucold_iwarm = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_test_ucold_iwarm.csv')\n",
    "y_test_ucold_icold = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_test_ucold_icold.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed data with TabPFN predictions\n",
    "X_train_combined = hstack([X_train_transformed, csr_matrix(tabpfn_preds.reshape(-1, 1))])\n",
    "X_val_combined = hstack([X_val_transformed, csr_matrix(tabpfn_preds.reshape(-1, 1))])\n",
    "X_test_uwarm_iwarm_combined = hstack([X_test_uwarm_iwarm_transformed, csr_matrix(tabpfn_preds.reshape(-1, 1))])\n",
    "X_test_uwarm_icold_combined = hstack([X_test_uwarm_icold_transformed, csr_matrix(tabpfn_preds.reshape(-1, 1))])\n",
    "X_test_ucold_iwarm_combined = hstack([X_test_ucold_iwarm_transformed, csr_matrix(tabpfn_preds.reshape(-1, 1))])\n",
    "X_test_ucold_icold_combined = hstack([X_test_ucold_icold_transformed, csr_matrix(tabpfn_preds.reshape(-1, 1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TabPFN predictions as a feature in LGBM\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.fit(X_train_combined, y_train)\n",
    "y_pred_uwarm_iwarm = lgb_model.predict(X_test_uwarm_iwarm_combined)\n",
    "y_pred_uwarm_icold = lgb_model.predict(X_test_uwarm_icold_combined)\n",
    "y_pred_ucold_iwarm = lgb_model.predict(X_test_ucold_iwarm_combined)\n",
    "y_pred_ucold_icold = lgb_model.predict(X_test_ucold_icold_combined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "# Calculate MSE for each test set\n",
    "mse_uwarm_iwarm = mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mse_uwarm_icold = mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Calculate RMSE for each test set\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Calculate MAE for each test set\n",
    "mae_uwarm_iwarm = mean_absolute_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Print the results\n",
    "print(f'MSE for warm user warm item: {mse_uwarm_iwarm:.4f}')\n",
    "print(f'RMSE for warm user warm item: {rmse_uwarm_iwarm:.4f}')\n",
    "print(f'MAE for warm user warm item: {mae_uwarm_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for warm user cold item: {mse_uwarm_icold:.4f}')\n",
    "print(f'RMSE for warm user cold item: {rmse_uwarm_icold:.4f}')\n",
    "print(f'MAE for warm user cold item: {mae_uwarm_icold:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user warm item: {mse_ucold_iwarm:.4f}')\n",
    "print(f'RMSE for cold user warm item: {rmse_ucold_iwarm:.4f}')\n",
    "print(f'MAE for cold user warm item: {mae_ucold_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user cold item: {mse_ucold_icold:.4f}')\n",
    "print(f'RMSE for cold user cold item: {rmse_ucold_icold:.4f}')\n",
    "print(f'MAE for cold user cold item: {mae_ucold_icold:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train plain LGBM model\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.fit(X_train_transformed, y_train)\n",
    "y_pred_uwarm_iwarm = lgb_model.predict(X_test_uwarm_iwarm_transformed)\n",
    "y_pred_uwarm_icold = lgb_model.predict(X_test_uwarm_icold_transformed)\n",
    "y_pred_ucold_iwarm = lgb_model.predict(X_test_ucold_iwarm_transformed)\n",
    "y_pred_ucold_icold = lgb_model.predict(X_test_ucold_icold_transformed)\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "# Calculate MSE for each test set\n",
    "mse_uwarm_iwarm = mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mse_uwarm_icold = mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Calculate RMSE for each test set\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Calculate MAE for each test set\n",
    "mae_uwarm_iwarm = mean_absolute_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Print the results\n",
    "print(f'MSE for warm user warm item: {mse_uwarm_iwarm:.4f}')\n",
    "print(f'RMSE for warm user warm item: {rmse_uwarm_iwarm:.4f}')\n",
    "print(f'MAE for warm user warm item: {mae_uwarm_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for warm user cold item: {mse_uwarm_icold:.4f}')\n",
    "print(f'RMSE for warm user cold item: {rmse_uwarm_icold:.4f}')\n",
    "print(f'MAE for warm user cold item: {mae_uwarm_icold:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user warm item: {mse_ucold_iwarm:.4f}')\n",
    "print(f'RMSE for cold user warm item: {rmse_ucold_iwarm:.4f}')\n",
    "print(f'MAE for cold user warm item: {mae_ucold_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user cold item: {mse_ucold_icold:.4f}')\n",
    "print(f'RMSE for cold user cold item: {rmse_ucold_icold:.4f}')\n",
    "print(f'MAE for cold user cold item: {mae_ucold_icold:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
