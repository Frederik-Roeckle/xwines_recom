{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc02e9e6",
   "metadata": {},
   "source": [
    "# Hybrid recommender\n",
    "## Collaborative filtering + Content-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf1f93",
   "metadata": {},
   "source": [
    "## Plan:\n",
    "* Load and merge the wines metadata on WineID.\n",
    "* Load the data, format columns to the correct types like:\n",
    "    * Grapes and Harmonize (string list) -> (python list)  \n",
    "    !!! This step is required in future always when loading this columns from csv.\n",
    "    * Vintage (str) -> numeric. I just simply replace N.V.(non-vintage) with 0 and then turn whole column to integer type.\n",
    "    * Datetime to a proper pd.datetime type\n",
    "* Columns used:\n",
    "    * **WineID**: Integer. The wine primary key identification;\n",
    "    * **WineName**: String. The textual wine identification presented in the label;\n",
    "    * **Type**: String. The categorical type classification: Red, white or rosé for still wines, gasified sparkling or dessert for sweeter and fortified wines. Dessert/Port is a subclassification for liqueur dessert wines;\n",
    "    * **Elaborate**: String. Categorical classification between varietal or assemblage/blend. The most famous blends are also considered, such as * Bordeaux red and white blend, Valpolicella blend and Portuguese red and white blend;\n",
    "    * **Grapes**: String list. It contains the grape varieties used in the wine elaboration. The original names found have been kept;\n",
    "    * **Harmonize**: String list. It contains the main dishes set that pair with the wine item. These are provided by producers but openly recommended on the internet by sommeliers and even consumers;\n",
    "    * **ABV**: Float. The alcohol by volume (ABV) percentage. According to [1], the value shown on the label may vary, and a tolerance of 0.5% per 100 volume is allowed, reaching 0.8% for some wines;\n",
    "    * **Body**: String. The categorical body classification: Very light-bodied, light-bodied, medium-bodied, full-bodied or very full-bodied based on wine viscosity [37];\n",
    "    * **Acidity**: String. The categorical acidity classification: Low, medium, or high, based on potential hydrogen (pH) score [38];\n",
    "    * **Country**: String. The categorical origin country identification of the wine production (ISO-3166);\n",
    "    * **RegionName**: String. The textual wine region identification. The appellation region name was retained when identified;\n",
    "    * **WineryName**: String. The textual winery identification;\n",
    "    * **UserID**: Integer. The sequential key without identifying the user's private data;\n",
    "    * **Vintage**: String. A rated vintage year or the abbreviation \"N.V.\" referring to \"non-vintage\";\n",
    "    * **Date**: String. Datetime in the format YYYY-MM-DD hh:mm:ss informing when it was rated by the user. It can be easily converted to other formats.\n",
    "    * **Rating**(**Target variable**): Float. It contains the 5-stars (1–5) rating value ⊂ {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5} performed by the user;\n",
    "* Columns dropped:\n",
    "    * **RegionID** - since it is just unique IDs and not descriptive for the recommender.\n",
    "    * **Code** - since it's the same meaning as **Country** column. Either one can be selected.\n",
    "    * **Vintages** - since it's just lists of possible vintages and we already have a Vintage column with the exact value(year or 0 for non-vintage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8ee3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# Training and evaluation\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from category_encoders import TargetEncoder, HashingEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack, csr_matrix, issparse, lil_matrix, save_npz, load_npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dcacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter of string lists into Python lists\n",
    "# (e.g. \"['a', 'b', 'c']\" → [a, b, c])\n",
    "def parse_list_col(s):\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Converter of 'N.V.' to 0, so column is numeric\n",
    "def parse_vintage(s):\n",
    "    return 0 if s == 'N.V.' else int(s)\n",
    "\n",
    "\n",
    "base_path = '..\\..\\..\\..\\data\\main'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd42175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test splits\n",
    "\n",
    "wines = pd.read_csv(\n",
    "    f'{base_path}\\\\XWines_Full_100K_wines.csv', \n",
    "    usecols=['WineID', 'Type', 'Elaborate', 'ABV', 'Body', 'Acidity', 'RegionName', 'WineryName', 'Grapes','Harmonize','Country'],\n",
    "    converters={\n",
    "        'Grapes':    parse_list_col,\n",
    "        'Harmonize': parse_list_col\n",
    "    }\n",
    ")\n",
    "train = pd.read_csv(\n",
    "    f'{base_path}\\\\trainset.csv', \n",
    "    usecols=['UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_uwarm_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_warm_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_uwarm_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_warm_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_ucold_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_cold_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_ucold_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_cold_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532745ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (16917894, 15)\n",
      "Test warm user warm item shape: (2036778, 16)\n",
      "Test warm user cold item shape: (35456, 16)\n",
      "Test cold user warm item shape: (506800, 16)\n",
      "Test cold user cold item shape: (16504, 16)\n"
     ]
    }
   ],
   "source": [
    "# Merge ratings with wines metadata on 'WineID'\n",
    "train = train.merge(wines, on='WineID', how='left')\n",
    "test_uwarm_iwarm = test_uwarm_iwarm.merge(wines, on='WineID', how='left')\n",
    "test_uwarm_icold = test_uwarm_icold.merge(wines, on='WineID', how='left')\n",
    "test_ucold_iwarm = test_ucold_iwarm.merge(wines, on='WineID', how='left')\n",
    "test_ucold_icold = test_ucold_icold.merge(wines, on='WineID', how='left')\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test warm user warm item shape: {test_uwarm_iwarm.shape}\")\n",
    "print(f\"Test warm user cold item shape: {test_uwarm_icold.shape}\")\n",
    "print(f\"Test cold user warm item shape: {test_ucold_iwarm.shape}\")\n",
    "print(f\"Test cold user cold item shape: {test_ucold_icold.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48fbf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small sample of the training set\n",
    "# train = train.sample(frac=0.1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12386489",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Preprocessing methods for different features:\n",
    "* **Standard Scaler** - is used for numerical type columns\n",
    "    * **ABV**\n",
    "    * **Vintage** (formatted to be numerical)\n",
    "    * **DaysAgo(Date)** - see below\n",
    "* **One-hot-encoding** - is used for categorical features, but is limited by the number of categories within a feature:\n",
    "    * **Type**\n",
    "    * **Body**\n",
    "    * **Acidity**\n",
    "    * **Elaborate**\n",
    "* **Multi-Label** - is used for categorical features with too many categories, where also multiple active categories could be possible:\n",
    "    * **Grapes** (774 classes)\n",
    "    * **Harmonize** (~64 classes)\n",
    "* **Target Encoding** - used for text features and user IDs, wine IDs. Used with KFold(5 folds) to prevent data leakage, i.e. encoded feature never knows about it's own target value\n",
    "* **Date encoding** - created custom object to convert datetime column to DaysAgo from the most recent record column. This way we keep information about time-related information and reduce feature to be simply numerical. **Standard Scaler** applied afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f793f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate features\n",
    "\n",
    "# Use StandardScaler for numerical features (create binary Is_NonVintage column derived from Vintage and maybe fill NaN values with 0)\n",
    "numerical_features = ['ABV', 'Vintage']\n",
    "# Use one-hot encoding for categorical features\n",
    "categorical_features = ['Type', 'Elaborate', 'Body', 'Acidity']\n",
    "# Use multilabel binarizer for multilabel features\n",
    "multilabel_features = ['Grapes', 'Harmonize']\n",
    "# Use target encoding for Country\n",
    "targetencoder_features = ['Country']\n",
    "# Use frequency encoder for IDs and high cardinality features\n",
    "frequency_features = ['WineID', 'UserID', 'WineryName', 'RegionName']\n",
    "# Use conversion to DaysAgo for time-based features\n",
    "date_features = ['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ad16f",
   "metadata": {},
   "source": [
    "* **Create Preprocessing pipeline**:\n",
    "    ##### **Important**: Since during Grapes column encoding we create 774 classes + there are around 100 classes from other encoders, the pandas DataFrame would require too much RAM (I recieved 69GB memory allocation error only for the Grapes column) and same happening for the dense array (numpy), I used csr_matrix from scipy.sparse and some additional optimizations for MultiLabelBinarizer in particular, to be able to store all the preprocessed features. More info in code below.\n",
    "\n",
    "    * **Created custom object for Date column preprocessing**\n",
    "    * **Created Wrappers for other preprocessors to always return sparse csr matricies**. For OneHotEncoding there is already implemented sparse output. For StandardScaler in date_pipeline wrapper is not required, since the input is already a csr matrix.\n",
    "    * **Created pipelines for each preprocessor and a general pipeline to combine everything together, using ColumnTransformer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "075e413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date preprocessor\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transforms a single datetime column into 'days ago' relative to the latest date in training data.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Expect a DataFrame with a single datetime column\n",
    "        self.col = X.columns[0]\n",
    "        self.column = f\"{self.col}_days_ago\"\n",
    "        self.reference_date = pd.to_datetime(X[self.col]).max()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        days_ago = (self.reference_date - pd.to_datetime(X[self.col])).dt.days\n",
    "        \n",
    "        return csr_matrix(days_ago.values.reshape(-1, 1))\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array([self.column])\n",
    "    \n",
    "\n",
    "class MultiLabelWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = []\n",
    "        for col in X.columns:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            # Fill missing with empty list for consistent fitting\n",
    "            safe_col = X[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "            mlb.fit(safe_col)\n",
    "            self.encoders[col] = mlb\n",
    "            self.feature_names.extend([f\"{col}__{cls}\" for cls in mlb.classes_])\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        matricies = []\n",
    "        for col in X.columns:\n",
    "            mlb = self.encoders[col]\n",
    "            class_index = {cls: i for i, cls in enumerate(mlb.classes_)}\n",
    "            n_rows = len(X)\n",
    "            n_classes = len(mlb.classes_)\n",
    "            sparse = lil_matrix((n_rows, n_classes), dtype=np.uint8)\n",
    "\n",
    "            for i, labels in enumerate(X[col]):\n",
    "                # Handle missing or malformed entries\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = []\n",
    "                for label in labels:\n",
    "                    if label in class_index:\n",
    "                        sparse[i, class_index[label]] = 1\n",
    "\n",
    "            matricies.append(sparse.tocsr())\n",
    "        return hstack(matricies, format='csr')\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "\n",
    "class TargetEncoderWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.feature_names = []\n",
    "        self.global_means = {}\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for col in X.columns:\n",
    "            te = TargetEncoder(cols=[col])\n",
    "            te.fit(X[[col]], y, cv=kf)\n",
    "            self.encoders[col] = te\n",
    "            self.global_means[col] = y.mean() \n",
    "            self.feature_names.append(f'{col}_target_encoded')\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        matricies = []\n",
    "        for col in X.columns:\n",
    "            te = self.encoders[col]\n",
    "            df_encoded = te.transform(X[[col]])\n",
    "            arr = df_encoded[col].fillna(self.global_means[col]).values.reshape(-1, 1)\n",
    "            matricies.append(csr_matrix(arr))\n",
    "        return hstack(matricies, format='csr')\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "    \n",
    "    \n",
    "class FrequencyEncoderWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.freq_maps = {}\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.freq_maps = {}\n",
    "        self.feature_names = [f\"{col}_freq\" for col in X.columns]\n",
    "        for col in X.columns:\n",
    "            self.freq_maps[col] = X[col].value_counts(normalize=True).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        matrices = []\n",
    "        for col in X.columns:\n",
    "            freq = X[col].map(self.freq_maps[col]).fillna(0).values.reshape(-1, 1)\n",
    "            matrices.append(csr_matrix(freq))\n",
    "        return hstack(matrices, format='csr')\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "\n",
    "class StandardScalerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler(with_mean=False)\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        # Always return sparse csr_matrix\n",
    "        if not issparse(X_scaled):\n",
    "            X_scaled = csr_matrix(X_scaled)\n",
    "        return X_scaled\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "## Preprocessing pipeline\n",
    "\n",
    "# Numerical\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('scaler', StandardScalerWrapper()),\n",
    "    ])\n",
    "\n",
    "# Categorical via one-hot-encoding\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "\n",
    "# Categorical via MultiLabelBinarizer\n",
    "multilabel_pipeline = Pipeline([\n",
    "    ('multilabel', MultiLabelWrapper())\n",
    "])\n",
    "\n",
    "# Country via target encoding\n",
    "target_pipeline = Pipeline([\n",
    "    ('target', TargetEncoderWrapper())\n",
    "])\n",
    "\n",
    "# High cardinality categorical features via frequency encoding\n",
    "# (i.e. WineryName, RegionName, UserID, WineID)\n",
    "frequency_pipeline = Pipeline([\n",
    "    ('frequency', FrequencyEncoderWrapper())\n",
    "])\n",
    "\n",
    "# Datetime via custom date transformer\n",
    "date_pipeline = Pipeline([\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "# Preprocessor\n",
    "# Remainnder contains RatingID column, which is not needed for training neither for testing\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numerical', numerical_pipeline, numerical_features),\n",
    "    ('categorical', categorical_pipeline, categorical_features),\n",
    "    ('multilabel', multilabel_pipeline, multilabel_features),\n",
    "    ('target', target_pipeline, targetencoder_features),\n",
    "    ('frequency', frequency_pipeline, frequency_features),\n",
    "    ('date', date_pipeline, date_features)\n",
    "], remainder='drop')\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b82d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop target column for all datasets\n",
    "\n",
    "X_train = train.drop(columns=['Rating'])\n",
    "y_train = train['Rating']\n",
    "\n",
    "X_test_uwarm_iwarm = test_uwarm_iwarm.drop(columns=['Rating'])\n",
    "y_test_uwarm_iwarm = test_uwarm_iwarm['Rating']\n",
    "\n",
    "X_test_uwarm_icold = test_uwarm_icold.drop(columns=['Rating'])\n",
    "y_test_uwarm_icold = test_uwarm_icold['Rating']\n",
    "\n",
    "X_test_ucold_iwarm = test_ucold_iwarm.drop(columns=['Rating'])\n",
    "y_test_ucold_iwarm = test_ucold_iwarm['Rating']\n",
    "\n",
    "X_test_ucold_icold = test_ucold_icold.drop(columns=['Rating'])\n",
    "y_test_ucold_icold = test_ucold_icold['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bfceeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train/val split for hyperparameter tuning\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c89777",
   "metadata": {},
   "source": [
    "* **Fit preprocessing pipeline on training data. We pass target variable there for the Target Encoder**\n",
    "* **Transform train and test sets on a fitted preprocessor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732a405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names size: 884\n",
      "Transformed train data size: 884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit the preprocessing pipeline\n",
    "preprocessing_pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_train_transformed = preprocessing_pipeline.transform(X_train)\n",
    "X_val_transformed = preprocessing_pipeline.transform(X_val)\n",
    "X_test_uwarm_iwarm_transformed = preprocessing_pipeline.transform(X_test_uwarm_iwarm)\n",
    "X_test_uwarm_icold_transformed = preprocessing_pipeline.transform(X_test_uwarm_icold)\n",
    "X_test_ucold_iwarm_transformed = preprocessing_pipeline.transform(X_test_ucold_iwarm)\n",
    "X_test_ucold_icold_transformed = preprocessing_pipeline.transform(X_test_ucold_icold)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = preprocessing_pipeline.get_feature_names_out()\n",
    "# Check the size of feature names and transformed data features \n",
    "print(f\"Feature names size: {len(feature_names)}\")\n",
    "print(f\"Transformed train data size: {X_train_transformed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7647c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save transformed data to npz\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_train_transformed.npz', X_train_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_val_transformed.npz', X_val_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_test_uwarm_iwarm_transformed.npz', X_test_uwarm_iwarm_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_test_uwarm_icold_transformed.npz', X_test_uwarm_icold_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_test_ucold_iwarm_transformed.npz', X_test_ucold_iwarm_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_test_ucold_icold_transformed.npz', X_test_ucold_icold_transformed)\n",
    "# Save target values to csv\n",
    "y_train.to_csv(f'{base_path}\\\\preprocessed\\\\y_train.csv', index=False)\n",
    "y_val.to_csv(f'{base_path}\\\\preprocessed\\\\y_val.csv', index=False)\n",
    "y_test_uwarm_iwarm.to_csv(f'{base_path}\\\\preprocessed\\\\y_test_uwarm_iwarm.csv', index=False)\n",
    "y_test_uwarm_icold.to_csv(f'{base_path}\\\\preprocessed\\\\y_test_uwarm_icold.csv', index=False)\n",
    "y_test_ucold_iwarm.to_csv(f'{base_path}\\\\preprocessed\\\\y_test_ucold_iwarm.csv', index=False)\n",
    "y_test_ucold_icold.to_csv(f'{base_path}\\\\preprocessed\\\\y_test_ucold_icold.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208a61a",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48c57e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load transformed data from npz\n",
    "# X_train_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_train_transformed.npz')\n",
    "# X_val_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_val_transformed.npz')\n",
    "# X_test_uwarm_iwarm_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_test_uwarm_iwarm_transformed.npz')\n",
    "# X_test_uwarm_icold_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_test_uwarm_icold_transformed.npz')\n",
    "# X_test_ucold_iwarm_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_test_ucold_iwarm_transformed.npz')\n",
    "# X_test_ucold_icold_transformed = load_npz(f'{base_path}\\\\preprocessed\\\\X_test_ucold_icold_transformed.npz')\n",
    "\n",
    "# # Load target variables\n",
    "# y_train = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_train.csv')\n",
    "# y_val = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_val.csv')\n",
    "# y_test_uwarm_iwarm = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_test_uwarm_iwarm.csv')\n",
    "# y_test_uwarm_icold = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_test_uwarm_icold.csv')\n",
    "# y_test_ucold_iwarm = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_test_ucold_iwarm.csv')\n",
    "# y_test_ucold_icold = pd.read_csv(f'{base_path}\\\\preprocessed\\\\y_test_ucold_icold.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfea51f",
   "metadata": {},
   "source": [
    "## LightGBM model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ad505ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 12.303795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2917\n",
      "[LightGBM] [Info] Number of data points in the train set: 13534315, number of used features: 728\n",
      "[LightGBM] [Info] Start training from score 3.858934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train a LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.fit(X_train_transformed, y_train, feature_name=list(feature_names))\n",
    "\n",
    "y_pred_uwarm_iwarm = lgb_model.predict(X_test_uwarm_iwarm_transformed)\n",
    "y_pred_uwarm_icold = lgb_model.predict(X_test_uwarm_icold_transformed)\n",
    "y_pred_ucold_iwarm = lgb_model.predict(X_test_ucold_iwarm_transformed)\n",
    "y_pred_ucold_icold = lgb_model.predict(X_test_ucold_icold_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d57878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the predictions as RatingID, PredictedRating\n",
    "\n",
    "# Warm user warm item\n",
    "result_uwarm_iwarm = pd.DataFrame({\n",
    "    'RatingID': test_uwarm_iwarm['RatingID'],\n",
    "    'Rating': y_pred_uwarm_iwarm\n",
    "})\n",
    "result_uwarm_iwarm.to_csv(\n",
    "    f'{base_path}\\\\lightgbm\\\\lightgbm_warm_user_warm_item.csv', \n",
    "    index=False, \n",
    "    header=['RatingID', 'Rating']\n",
    ")\n",
    "# Warm user cold item\n",
    "result_uwarm_icold = pd.DataFrame({\n",
    "    'RatingID': test_uwarm_icold['RatingID'],\n",
    "    'Rating': y_pred_uwarm_icold\n",
    "})\n",
    "result_uwarm_icold.to_csv(\n",
    "    f'{base_path}\\\\lightgbm\\\\lightgbm_warm_user_cold_item.csv', \n",
    "    index=False, \n",
    "    header=['RatingID', 'Rating']\n",
    ")\n",
    "# Cold user warm item\n",
    "result_ucold_iwarm = pd.DataFrame({\n",
    "    'RatingID': test_ucold_iwarm['RatingID'],\n",
    "    'Rating': y_pred_ucold_iwarm\n",
    "})\n",
    "result_ucold_iwarm.to_csv(\n",
    "    f'{base_path}\\\\lightgbm\\\\lightgbm_cold_user_warm_item.csv', \n",
    "    index=False, \n",
    "    header=['RatingID', 'Rating']\n",
    ")\n",
    "# Cold user cold item\n",
    "result_ucold_icold = pd.DataFrame({\n",
    "    'RatingID': test_ucold_icold['RatingID'],\n",
    "    'Rating': y_pred_ucold_icold\n",
    "})\n",
    "result_ucold_icold.to_csv(\n",
    "    f'{base_path}\\\\lightgbm\\\\lightgbm_cold_user_cold_item.csv', \n",
    "    index=False, \n",
    "    header=['RatingID', 'Rating']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "781c6b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for warm user warm item: 0.4031\n",
      "RMSE for warm user warm item: 0.6349\n",
      "MAE for warm user warm item: 0.4711\n",
      "--------------------------------------------------\n",
      "MSE for warm user cold item: 0.4447\n",
      "RMSE for warm user cold item: 0.6668\n",
      "MAE for warm user cold item: 0.5031\n",
      "--------------------------------------------------\n",
      "MSE for cold user warm item: 0.5012\n",
      "RMSE for cold user warm item: 0.7080\n",
      "MAE for cold user warm item: 0.5233\n",
      "--------------------------------------------------\n",
      "MSE for cold user cold item: 0.5852\n",
      "RMSE for cold user cold item: 0.7650\n",
      "MAE for cold user cold item: 0.5821\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "# Calculate MSE for each test set\n",
    "mse_uwarm_iwarm = mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mse_uwarm_icold = mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Calculate RMSE for each test set\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Calculate MAE for each test set\n",
    "mae_uwarm_iwarm = mean_absolute_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "\n",
    "# Print the results\n",
    "print(f'MSE for warm user warm item: {mse_uwarm_iwarm:.4f}')\n",
    "print(f'RMSE for warm user warm item: {rmse_uwarm_iwarm:.4f}')\n",
    "print(f'MAE for warm user warm item: {mae_uwarm_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for warm user cold item: {mse_uwarm_icold:.4f}')\n",
    "print(f'RMSE for warm user cold item: {rmse_uwarm_icold:.4f}')\n",
    "print(f'MAE for warm user cold item: {mae_uwarm_icold:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user warm item: {mse_ucold_iwarm:.4f}')\n",
    "print(f'RMSE for cold user warm item: {rmse_ucold_iwarm:.4f}')\n",
    "print(f'MAE for cold user warm item: {mae_ucold_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user cold item: {mse_ucold_icold:.4f}')\n",
    "print(f'RMSE for cold user cold item: {rmse_ucold_icold:.4f}')\n",
    "print(f'MAE for cold user cold item: {mae_ucold_icold:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8203f0",
   "metadata": {},
   "source": [
    "## XGBoost model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a4c8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with DMatrix\n",
    "dtrain = xgb.DMatrix(X_train_transformed, label=y_train, feature_names=list(feature_names))\n",
    "xgb_model = xgb.train(\n",
    "    params={\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': 42\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=100\n",
    ")\n",
    "\n",
    "d_test_uwarm_iwarm = xgb.DMatrix(X_test_uwarm_iwarm_transformed, feature_names=list(feature_names))\n",
    "d_test_uwarm_icold = xgb.DMatrix(X_test_uwarm_icold_transformed, feature_names=list(feature_names))\n",
    "d_test_ucold_iwarm = xgb.DMatrix(X_test_ucold_iwarm_transformed, feature_names=list(feature_names))\n",
    "d_test_ucold_icold = xgb.DMatrix(X_test_ucold_icold_transformed, feature_names=list(feature_names))\n",
    "\n",
    "# Predict using the trained model\n",
    "y_pred_uwarm_iwarm = xgb_model.predict(d_test_uwarm_iwarm)\n",
    "y_pred_uwarm_icold = xgb_model.predict(d_test_uwarm_icold)\n",
    "y_pred_ucold_iwarm = xgb_model.predict(d_test_ucold_iwarm)\n",
    "y_pred_ucold_icold = xgb_model.predict(d_test_ucold_icold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "400171b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the predictions as RatingID, PredictedRating\n",
    "# Warm user warm item\n",
    "result_uwarm_iwarm = pd.DataFrame({\n",
    "    'RatingID': test_uwarm_iwarm['RatingID'],\n",
    "    'Rating': y_pred_uwarm_iwarm\n",
    "})\n",
    "result_uwarm_iwarm.to_csv(\n",
    "    f'{base_path}\\\\xgboost\\\\xgboost_warm_user_warm_item.csv', \n",
    "    index=False, \n",
    "    header=['RatingID', 'Rating']\n",
    ")\n",
    "# Warm user cold item\n",
    "result_uwarm_icold = pd.DataFrame({\n",
    "    'RatingID': test_uwarm_icold['RatingID'],\n",
    "    'Rating': y_pred_uwarm_icold\n",
    "})\n",
    "result_uwarm_icold.to_csv(\n",
    "    f'{base_path}\\\\xgboost\\\\xgboost_warm_user_cold_item.csv', \n",
    "    index=False, \n",
    "    header=['RatingID', 'Rating']\n",
    ")\n",
    "# Cold user warm item\n",
    "result_ucold_iwarm = pd.DataFrame({\n",
    "    'RatingID': test_ucold_iwarm['RatingID'],\n",
    "    'Rating': y_pred_ucold_iwarm\n",
    "})\n",
    "result_ucold_iwarm.to_csv(\n",
    "    f'{base_path}\\\\xgboost\\\\xgboost_cold_user_warm_item.csv', \n",
    "    index=False, \n",
    "    header=['RatingID', 'Rating']\n",
    ")\n",
    "# Cold user cold item\n",
    "result_ucold_icold = pd.DataFrame({\n",
    "    'RatingID': test_ucold_icold['RatingID'],\n",
    "    'Rating': y_pred_ucold_icold\n",
    "})\n",
    "result_ucold_icold.to_csv(\n",
    "    f'{base_path}\\\\xgboost\\\\xgboost_cold_user_cold_item.csv', \n",
    "    index=False, \n",
    "    header=['RatingID', 'Rating']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1af764b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for warm user warm item: 0.3974\n",
      "RMSE for warm user warm item: 0.6304\n",
      "MAE for warm user warm item: 0.4677\n",
      "--------------------------------------------------\n",
      "MSE for warm user cold item: 0.5654\n",
      "RMSE for warm user cold item: 0.7519\n",
      "MAE for warm user cold item: 0.5578\n",
      "--------------------------------------------------\n",
      "MSE for cold user warm item: 0.4983\n",
      "RMSE for cold user warm item: 0.7059\n",
      "MAE for cold user warm item: 0.5247\n",
      "--------------------------------------------------\n",
      "MSE for cold user cold item: 0.8514\n",
      "RMSE for cold user cold item: 0.9227\n",
      "MAE for cold user cold item: 0.6975\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "# Calculate MSE for each test set\n",
    "mse_uwarm_iwarm = mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mse_uwarm_icold = mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# Calculate RMSE for each test set\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# Calculate MAE for each test set\n",
    "mae_uwarm_iwarm = mean_absolute_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# Print the results\n",
    "print(f'MSE for warm user warm item: {mse_uwarm_iwarm:.4f}')\n",
    "print(f'RMSE for warm user warm item: {rmse_uwarm_iwarm:.4f}')\n",
    "print(f'MAE for warm user warm item: {mae_uwarm_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for warm user cold item: {mse_uwarm_icold:.4f}')\n",
    "print(f'RMSE for warm user cold item: {rmse_uwarm_icold:.4f}')\n",
    "print(f'MAE for warm user cold item: {mae_uwarm_icold:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user warm item: {mse_ucold_iwarm:.4f}')\n",
    "print(f'RMSE for cold user warm item: {rmse_ucold_iwarm:.4f}')\n",
    "print(f'MAE for cold user warm item: {mae_ucold_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user cold item: {mse_ucold_icold:.4f}')\n",
    "print(f'RMSE for cold user cold item: {rmse_ucold_icold:.4f}')\n",
    "print(f'MAE for cold user cold item: {mae_ucold_icold:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060542f",
   "metadata": {},
   "source": [
    "# Evaluate top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2dfc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_uwarm_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\xgboost\\\\xgboost_warm_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'Rating']\n",
    ")\n",
    "results_uwarm_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\xgboost\\\\xgboost_warm_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'Rating']\n",
    ")\n",
    "results_ucold_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\xgboost\\\\xgboost_cold_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'Rating']\n",
    ")\n",
    "results_ucold_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\xgboost\\\\xgboost_cold_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'Rating']\n",
    ")\n",
    "\n",
    "# Load the test set\n",
    "test_uwarm_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_warm_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating']\n",
    ")\n",
    "test_uwarm_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_warm_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating']\n",
    ")\n",
    "test_ucold_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_cold_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating']\n",
    ")\n",
    "test_ucold_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_cold_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating']\n",
    ")\n",
    "# Merge the results with the test set\n",
    "results_uwarm_iwarm = results_uwarm_iwarm.merge(test_uwarm_iwarm, on='RatingID', how='left')\n",
    "results_uwarm_icold = results_uwarm_icold.merge(test_uwarm_icold, on='RatingID', how='left')\n",
    "results_ucold_iwarm = results_ucold_iwarm.merge(test_ucold_iwarm, on='RatingID', how='left')\n",
    "results_ucold_icold = results_ucold_icold.merge(test_ucold_icold, on='RatingID', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d3c9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Rank and Rank_pred columns\n",
    "\n",
    "# Warm user, warm item\n",
    "results_uwarm_iwarm[\"Rank\"] = results_uwarm_iwarm.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_uwarm_iwarm[\"Rank_pred\"] = results_uwarm_iwarm.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Warm user, cold item\n",
    "results_uwarm_icold[\"Rank\"] = results_uwarm_icold.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_uwarm_icold[\"Rank_pred\"] = results_uwarm_icold.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Cold user, warm item\n",
    "results_ucold_iwarm[\"Rank\"] = results_ucold_iwarm.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_ucold_iwarm[\"Rank_pred\"] = results_ucold_iwarm.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "# Cold user, cold item\n",
    "results_ucold_icold[\"Rank\"] = results_ucold_icold.groupby(\"UserID\")[\"Rating_y\"].rank(method=\"first\", ascending=False)\n",
    "results_ucold_icold[\"Rank_pred\"] = results_ucold_icold.groupby(\"UserID\")[\"Rating_x\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "# Calculate Relevance\n",
    "results_uwarm_iwarm[\"Relevance\"] = results_uwarm_iwarm[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_uwarm_icold[\"Relevance\"] = results_uwarm_icold[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_ucold_iwarm[\"Relevance\"] = results_ucold_iwarm[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "results_ucold_icold[\"Relevance\"] = results_ucold_icold[\"Rating_y\"].apply(lambda x: 1 if x >= 3.5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d56e331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_topk_fast(df, k=10):\n",
    "    # Pre-sort so top-k is at the top per user\n",
    "    df = df.sort_values(['UserID', 'Rank_pred'], ascending=[True, True])\n",
    "\n",
    "    # Assign group index per row (unique integer per user)\n",
    "    user_index, user_pos = np.unique(df['UserID'], return_inverse=True)\n",
    "\n",
    "    # Count items per user\n",
    "    user_counts = np.bincount(user_pos)\n",
    "    user_offsets = np.zeros(len(df), dtype=int)\n",
    "    np.add.at(user_offsets, np.cumsum(user_counts)[:-1], 1)\n",
    "    user_offsets = np.cumsum(user_offsets)\n",
    "\n",
    "    # Mask to keep only top-k per user\n",
    "    df['row_number'] = df.groupby('UserID').cumcount()\n",
    "    topk_df = df[df['row_number'] < k].copy()\n",
    "\n",
    "    # Precision@k\n",
    "    precision = topk_df['Relevance'].groupby(topk_df['UserID']).mean().mean()\n",
    "\n",
    "    # Recall@k\n",
    "    relevant_per_user = df.groupby('UserID')['Relevance'].sum()\n",
    "    hits_per_user = topk_df.groupby('UserID')['Relevance'].sum()\n",
    "    recall = (hits_per_user / relevant_per_user).fillna(0).mean()\n",
    "\n",
    "    # HitRate@k\n",
    "    hits = (hits_per_user > 0).astype(int)\n",
    "    hit_rate = hits.mean()\n",
    "\n",
    "    # MAP@k\n",
    "    def map_at_k_per_user(x):\n",
    "        rels = x['Relevance'].values\n",
    "        precisions = [(rels[:i + 1].sum() / (i + 1)) for i in range(len(rels)) if rels[i]]\n",
    "        return np.mean(precisions) if precisions else 0\n",
    "    mapk = topk_df.groupby('UserID').apply(map_at_k_per_user).mean()\n",
    "\n",
    "    # nDCG@k\n",
    "    def dcg(rels):\n",
    "        return np.sum(rels / np.log2(np.arange(2, len(rels) + 2)))\n",
    "    def ndcg_per_user(x):\n",
    "        dcg_val = dcg(x['Relevance'].values)\n",
    "        ideal = x.sort_values('Relevance', ascending=False).head(k)\n",
    "        idcg_val = dcg(ideal['Relevance'].values)\n",
    "        return dcg_val / idcg_val if idcg_val > 0 else 0\n",
    "    ndcg = topk_df.groupby('UserID').apply(ndcg_per_user).mean()\n",
    "\n",
    "    return {\n",
    "        'Precision@k': precision,\n",
    "        'Recall@k': recall,\n",
    "        'HitRate@k': hit_rate,\n",
    "        'MAP@k': mapk,\n",
    "        'nDCG@k': ndcg\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4447408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_5068\\1428188676.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mapk = topk_df.groupby('UserID').apply(map_at_k_per_user).mean()\n",
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_5068\\1428188676.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ndcg = topk_df.groupby('UserID').apply(ndcg_per_user).mean()\n",
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_5068\\1428188676.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mapk = topk_df.groupby('UserID').apply(map_at_k_per_user).mean()\n",
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_5068\\1428188676.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ndcg = topk_df.groupby('UserID').apply(ndcg_per_user).mean()\n",
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_5068\\1428188676.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mapk = topk_df.groupby('UserID').apply(map_at_k_per_user).mean()\n",
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_5068\\1428188676.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ndcg = topk_df.groupby('UserID').apply(ndcg_per_user).mean()\n",
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_5068\\1428188676.py:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  mapk = topk_df.groupby('UserID').apply(map_at_k_per_user).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on warm user, warm item at top 10:\n",
      "Precision@k: 0.8812\n",
      "Recall@k: 0.9232\n",
      "HitRate@k: 0.9526\n",
      "MAP@k: 0.9220\n",
      "nDCG@k: 0.9335\n",
      "--------------------------------------------------\n",
      "Evaluation on warm user, cold item at top 10:\n",
      "Precision@k: 0.8389\n",
      "Recall@k: 0.8605\n",
      "HitRate@k: 0.8605\n",
      "MAP@k: 0.8500\n",
      "nDCG@k: 0.8531\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, warm item at top 10:\n",
      "Precision@k: 0.8171\n",
      "Recall@k: 0.9218\n",
      "HitRate@k: 0.9930\n",
      "MAP@k: 0.9048\n",
      "nDCG@k: 0.9417\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, cold item at top 10:\n",
      "Precision@k: 0.7089\n",
      "Recall@k: 0.7210\n",
      "HitRate@k: 0.7210\n",
      "MAP@k: 0.7157\n",
      "nDCG@k: 0.7171\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_5068\\1428188676.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ndcg = topk_df.groupby('UserID').apply(ndcg_per_user).mean()\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "k = 10\n",
    "results = {}\n",
    "results['warm user, warm item'] = evaluate_topk_fast(results_uwarm_iwarm, k=k)\n",
    "results['warm user, cold item'] = evaluate_topk_fast(results_uwarm_icold, k=k)\n",
    "results['cold user, warm item'] = evaluate_topk_fast(results_ucold_iwarm, k=k)\n",
    "results['cold user, cold item'] = evaluate_topk_fast(results_ucold_icold, k=k)\n",
    "\n",
    "# Print evaluation results\n",
    "for case, metrics in results.items():\n",
    "    print(f\"Evaluation on {case} at top {k}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a941aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:17:31,481] A new study created in memory with name: no-name-95b4cc90-4d36-43cc-beb3-947f3bb48189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.460831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:20:17,359] Trial 0 finished with value: 0.46083116428832877 and parameters: {'learning_rate': 0.13045391082284233, 'num_leaves': 119, 'max_depth': 11, 'feature_fraction': 0.7644506725476632, 'bagging_fraction': 0.9258527730057012, 'lambda_l1': 3.858310278489699, 'lambda_l2': 8.326726434066856}. Best is trial 0 with value: 0.46083116428832877.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.488147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:22:57,493] Trial 1 finished with value: 0.4881468879232135 and parameters: {'learning_rate': 0.019777737861563657, 'num_leaves': 65, 'max_depth': 13, 'feature_fraction': 0.9388693825696973, 'bagging_fraction': 0.6466945952365604, 'lambda_l1': 3.2205831525455606, 'lambda_l2': 6.594478666684367}. Best is trial 0 with value: 0.46083116428832877.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.464115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:24:41,782] Trial 2 finished with value: 0.46411500095808705 and parameters: {'learning_rate': 0.19474868965916237, 'num_leaves': 43, 'max_depth': 11, 'feature_fraction': 0.7288937039575442, 'bagging_fraction': 0.6402005577562181, 'lambda_l1': 7.860344225679329, 'lambda_l2': 5.633758470053102}. Best is trial 0 with value: 0.46083116428832877.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.452286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:27:06,880] Trial 3 finished with value: 0.452286244393998 and parameters: {'learning_rate': 0.15814413689915602, 'num_leaves': 260, 'max_depth': 14, 'feature_fraction': 0.740975506150041, 'bagging_fraction': 0.7188835219382578, 'lambda_l1': 9.743894727018693, 'lambda_l2': 6.792800314545683}. Best is trial 3 with value: 0.452286244393998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.458627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:29:33,497] Trial 4 finished with value: 0.45862676301335553 and parameters: {'learning_rate': 0.10915788220475382, 'num_leaves': 162, 'max_depth': 14, 'feature_fraction': 0.8074632540566499, 'bagging_fraction': 0.695069513212854, 'lambda_l1': 8.85220703979081, 'lambda_l2': 2.3109636844337}. Best is trial 3 with value: 0.452286244393998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.45951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:32:23,662] Trial 5 finished with value: 0.45951021679515297 and parameters: {'learning_rate': 0.11223977782402415, 'num_leaves': 173, 'max_depth': 12, 'feature_fraction': 0.6503757085870779, 'bagging_fraction': 0.7283414849552249, 'lambda_l1': 8.102246973692763, 'lambda_l2': 7.192581559244634}. Best is trial 3 with value: 0.452286244393998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.47218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:35:31,981] Trial 6 finished with value: 0.47217994997562635 and parameters: {'learning_rate': 0.034909577194380585, 'num_leaves': 146, 'max_depth': 12, 'feature_fraction': 0.7525697163270872, 'bagging_fraction': 0.6516711363116362, 'lambda_l1': 0.0014736219575395282, 'lambda_l2': 2.77070908658429}. Best is trial 3 with value: 0.452286244393998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.459312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:37:49,513] Trial 7 finished with value: 0.4593122897919127 and parameters: {'learning_rate': 0.13794197871946656, 'num_leaves': 191, 'max_depth': 10, 'feature_fraction': 0.6473702240704083, 'bagging_fraction': 0.6728521583782838, 'lambda_l1': 1.0645535605146517, 'lambda_l2': 9.062763723638717}. Best is trial 3 with value: 0.452286244393998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.465695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:41:11,562] Trial 8 finished with value: 0.4656952297563073 and parameters: {'learning_rate': 0.04612994273991407, 'num_leaves': 219, 'max_depth': 12, 'feature_fraction': 0.6104200380438741, 'bagging_fraction': 0.8033584790815059, 'lambda_l1': 4.0440323803924425, 'lambda_l2': 7.601415335781261}. Best is trial 3 with value: 0.452286244393998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.455506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:43:45,040] Trial 9 finished with value: 0.4555055152173334 and parameters: {'learning_rate': 0.11376924209368486, 'num_leaves': 241, 'max_depth': 14, 'feature_fraction': 0.8712556437619501, 'bagging_fraction': 0.7837579168376795, 'lambda_l1': 8.927458059871508, 'lambda_l2': 5.03750873102343}. Best is trial 3 with value: 0.452286244393998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.467264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:45:11,105] Trial 10 finished with value: 0.46726380664375183 and parameters: {'learning_rate': 0.18159091720406087, 'num_leaves': 300, 'max_depth': 6, 'feature_fraction': 0.9917593118777853, 'bagging_fraction': 0.8925803633389914, 'lambda_l1': 6.396938802001518, 'lambda_l2': 0.5462316445068227}. Best is trial 3 with value: 0.452286244393998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.45047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:48:13,409] Trial 11 finished with value: 0.4504695007633011 and parameters: {'learning_rate': 0.15609282879099753, 'num_leaves': 278, 'max_depth': 16, 'feature_fraction': 0.8493414287851505, 'bagging_fraction': 0.7952777253929746, 'lambda_l1': 9.975690605396665, 'lambda_l2': 4.186645123364285}. Best is trial 11 with value: 0.4504695007633011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.449721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:51:22,908] Trial 12 finished with value: 0.44972070039428813 and parameters: {'learning_rate': 0.15592472750150818, 'num_leaves': 300, 'max_depth': 16, 'feature_fraction': 0.8478538250101956, 'bagging_fraction': 0.8504895325236643, 'lambda_l1': 9.666093633164055, 'lambda_l2': 3.5743798315364517}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.458259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:55:18,744] Trial 13 finished with value: 0.4582588884658774 and parameters: {'learning_rate': 0.07068963114621002, 'num_leaves': 288, 'max_depth': 16, 'feature_fraction': 0.8666125137197733, 'bagging_fraction': 0.9999738589191689, 'lambda_l1': 6.295737073132086, 'lambda_l2': 3.152030247447189}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.464957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:57:09,030] Trial 14 finished with value: 0.46495703131603566 and parameters: {'learning_rate': 0.1616434005367293, 'num_leaves': 260, 'max_depth': 7, 'feature_fraction': 0.8541098837095809, 'bagging_fraction': 0.8355637632550303, 'lambda_l1': 9.863297520603437, 'lambda_l2': 3.8193589095121836}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.487263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 20:58:21,504] Trial 15 finished with value: 0.487262645421536 and parameters: {'learning_rate': 0.0833742506595046, 'num_leaves': 214, 'max_depth': 3, 'feature_fraction': 0.9302631050394562, 'bagging_fraction': 0.8683656301618697, 'lambda_l1': 6.678330004362659, 'lambda_l2': 1.2425390197383201}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.450427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:01:26,232] Trial 16 finished with value: 0.45042739047787406 and parameters: {'learning_rate': 0.15720835983025633, 'num_leaves': 273, 'max_depth': 16, 'feature_fraction': 0.8168364711143219, 'bagging_fraction': 0.7700737566443343, 'lambda_l1': 7.527140875156801, 'lambda_l2': 4.016072482039324}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.450938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:04:31,433] Trial 17 finished with value: 0.45093824305251407 and parameters: {'learning_rate': 0.17749955959674515, 'num_leaves': 234, 'max_depth': 16, 'feature_fraction': 0.8057841199942966, 'bagging_fraction': 0.7531968895968679, 'lambda_l1': 7.546869251270179, 'lambda_l2': 1.6137422255539775}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.466224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:06:33,559] Trial 18 finished with value: 0.4662243797603351 and parameters: {'learning_rate': 0.1443708321732725, 'num_leaves': 89, 'max_depth': 7, 'feature_fraction': 0.9119010233685526, 'bagging_fraction': 0.9673417644876127, 'lambda_l1': 5.3165138710281585, 'lambda_l2': 5.553209864714651}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.456521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:09:55,611] Trial 19 finished with value: 0.45652136873254306 and parameters: {'learning_rate': 0.08585244306842389, 'num_leaves': 266, 'max_depth': 15, 'feature_fraction': 0.9899899574715232, 'bagging_fraction': 0.8489089763503894, 'lambda_l1': 8.591179769422238, 'lambda_l2': 9.962663471398297}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.458422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:12:16,338] Trial 20 finished with value: 0.4584220704443651 and parameters: {'learning_rate': 0.17628989950764756, 'num_leaves': 198, 'max_depth': 9, 'feature_fraction': 0.6945194411587525, 'bagging_fraction': 0.9265150040682585, 'lambda_l1': 7.145698792136635, 'lambda_l2': 4.228913611595041}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.450471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:15:30,917] Trial 21 finished with value: 0.450470670375464 and parameters: {'learning_rate': 0.15724755316017788, 'num_leaves': 282, 'max_depth': 16, 'feature_fraction': 0.8332924968956186, 'bagging_fraction': 0.7917591739679939, 'lambda_l1': 9.997188884819565, 'lambda_l2': 4.441799370859382}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.452796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:18:36,986] Trial 22 finished with value: 0.4527955673284349 and parameters: {'learning_rate': 0.13018484301660316, 'num_leaves': 297, 'max_depth': 15, 'feature_fraction': 0.8845147836509372, 'bagging_fraction': 0.6040193239997095, 'lambda_l1': 9.095984743567236, 'lambda_l2': 3.6868111445163505}. Best is trial 12 with value: 0.44972070039428813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.449458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:21:24,768] Trial 23 finished with value: 0.44945807184027836 and parameters: {'learning_rate': 0.19270501009662055, 'num_leaves': 251, 'max_depth': 15, 'feature_fraction': 0.7793483855756894, 'bagging_fraction': 0.8206422861447707, 'lambda_l1': 5.547639794671292, 'lambda_l2': 5.100626797993326}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.449717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:24:53,748] Trial 24 finished with value: 0.4497169172189235 and parameters: {'learning_rate': 0.19939315896988113, 'num_leaves': 245, 'max_depth': 15, 'feature_fraction': 0.7999337666784668, 'bagging_fraction': 0.8304905178692752, 'lambda_l1': 5.648721461894074, 'lambda_l2': 5.929890443056419}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.450169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:28:02,756] Trial 25 finished with value: 0.4501690931492071 and parameters: {'learning_rate': 0.1964048122691561, 'num_leaves': 239, 'max_depth': 14, 'feature_fraction': 0.7722539951733728, 'bagging_fraction': 0.8225007726565281, 'lambda_l1': 5.172499881604052, 'lambda_l2': 5.878746288648381}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.450892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:30:54,877] Trial 26 finished with value: 0.4508919964971002 and parameters: {'learning_rate': 0.19872116191931358, 'num_leaves': 244, 'max_depth': 13, 'feature_fraction': 0.7135944759016642, 'bagging_fraction': 0.8881100115935279, 'lambda_l1': 2.400312608473629, 'lambda_l2': 4.944856811035524}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.451428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:34:13,851] Trial 27 finished with value: 0.45142843459881726 and parameters: {'learning_rate': 0.18160284841870877, 'num_leaves': 216, 'max_depth': 15, 'feature_fraction': 0.7930556937250287, 'bagging_fraction': 0.8596509153068206, 'lambda_l1': 5.925126667459978, 'lambda_l2': 6.11998092588772}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.452396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:37:36,820] Trial 28 finished with value: 0.452395938069915 and parameters: {'learning_rate': 0.1717008375551741, 'num_leaves': 255, 'max_depth': 13, 'feature_fraction': 0.8988031342143481, 'bagging_fraction': 0.9104795027739171, 'lambda_l1': 4.585572755566475, 'lambda_l2': 7.867498313116075}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.462491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:40:02,742] Trial 29 finished with value: 0.4624909307042256 and parameters: {'learning_rate': 0.12791250486440564, 'num_leaves': 126, 'max_depth': 9, 'feature_fraction': 0.7738920993667691, 'bagging_fraction': 0.933079269750724, 'lambda_l1': 2.997014315868734, 'lambda_l2': 4.973470145429467}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.452068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:43:06,315] Trial 30 finished with value: 0.4520675927886856 and parameters: {'learning_rate': 0.185533458297902, 'num_leaves': 192, 'max_depth': 15, 'feature_fraction': 0.6848954091369032, 'bagging_fraction': 0.8212449687123392, 'lambda_l1': 4.201944577423665, 'lambda_l2': 2.8670382700063977}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.450268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:46:20,735] Trial 31 finished with value: 0.4502675602645327 and parameters: {'learning_rate': 0.19977311168456083, 'num_leaves': 232, 'max_depth': 14, 'feature_fraction': 0.7747267750103959, 'bagging_fraction': 0.8253259886312987, 'lambda_l1': 5.284923677728261, 'lambda_l2': 6.05608655576884}. Best is trial 23 with value: 0.44945807184027836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.449365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:49:48,484] Trial 32 finished with value: 0.44936544196071715 and parameters: {'learning_rate': 0.19020821286141062, 'num_leaves': 251, 'max_depth': 15, 'feature_fraction': 0.7780293627377767, 'bagging_fraction': 0.7521301101163334, 'lambda_l1': 4.772524582778525, 'lambda_l2': 6.79039850935762}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.449707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:52:49,068] Trial 33 finished with value: 0.44970654235827306 and parameters: {'learning_rate': 0.18829266168830902, 'num_leaves': 279, 'max_depth': 15, 'feature_fraction': 0.8277361783401412, 'bagging_fraction': 0.7522757749106084, 'lambda_l1': 3.530870032144615, 'lambda_l2': 6.735247854350033}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.45267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:55:45,890] Trial 34 finished with value: 0.4526698839968094 and parameters: {'learning_rate': 0.19014196836728586, 'num_leaves': 251, 'max_depth': 11, 'feature_fraction': 0.729022278684115, 'bagging_fraction': 0.7433677696750118, 'lambda_l1': 2.9477466962200602, 'lambda_l2': 6.878453162353491}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.453128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 21:59:02,102] Trial 35 finished with value: 0.45312804735846246 and parameters: {'learning_rate': 0.17138484869002016, 'num_leaves': 271, 'max_depth': 13, 'feature_fraction': 0.7901878521001778, 'bagging_fraction': 0.7078865041916251, 'lambda_l1': 3.3881742663910046, 'lambda_l2': 8.57283406988132}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.452945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:02:02,034] Trial 36 finished with value: 0.4529452960901806 and parameters: {'learning_rate': 0.16879554666698754, 'num_leaves': 224, 'max_depth': 14, 'feature_fraction': 0.8249835795765483, 'bagging_fraction': 0.7643252295553136, 'lambda_l1': 1.9889943028021246, 'lambda_l2': 7.373427975484681}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.464109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:04:06,599] Trial 37 finished with value: 0.4641093674472255 and parameters: {'learning_rate': 0.1862526875045782, 'num_leaves': 39, 'max_depth': 15, 'feature_fraction': 0.7310993491783473, 'bagging_fraction': 0.7309513414288394, 'lambda_l1': 5.701773657541641, 'lambda_l2': 6.583313160221854}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.456955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:07:04,580] Trial 38 finished with value: 0.4569553011708584 and parameters: {'learning_rate': 0.1473067728203028, 'num_leaves': 181, 'max_depth': 11, 'feature_fraction': 0.753720768834229, 'bagging_fraction': 0.678691627454951, 'lambda_l1': 4.40971362640455, 'lambda_l2': 8.073872264834275}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.452854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:10:07,901] Trial 39 finished with value: 0.45285444929523777 and parameters: {'learning_rate': 0.18895119453733, 'num_leaves': 204, 'max_depth': 12, 'feature_fraction': 0.7915532631488302, 'bagging_fraction': 0.8043588101888348, 'lambda_l1': 3.5760888769255827, 'lambda_l2': 6.50804666173381}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.455448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:12:37,519] Trial 40 finished with value: 0.4554479691458737 and parameters: {'learning_rate': 0.1664590576494036, 'num_leaves': 151, 'max_depth': 13, 'feature_fraction': 0.8311915129913011, 'bagging_fraction': 0.7684915450702613, 'lambda_l1': 4.949179868848883, 'lambda_l2': 5.442565083490085}. Best is trial 32 with value: 0.44936544196071715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.449144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:15:39,751] Trial 41 finished with value: 0.4491444240751035 and parameters: {'learning_rate': 0.19270944136663726, 'num_leaves': 290, 'max_depth': 15, 'feature_fraction': 0.8455080690450716, 'bagging_fraction': 0.8372887497075213, 'lambda_l1': 1.3127169173842077, 'lambda_l2': 4.667635782015954}. Best is trial 41 with value: 0.4491444240751035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.4488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:19:06,985] Trial 42 finished with value: 0.4488002186655424 and parameters: {'learning_rate': 0.19949864383408128, 'num_leaves': 283, 'max_depth': 15, 'feature_fraction': 0.8097140286131668, 'bagging_fraction': 0.8737349843116362, 'lambda_l1': 1.046040620381623, 'lambda_l2': 7.041416588234524}. Best is trial 42 with value: 0.4488002186655424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.450251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:22:40,028] Trial 43 finished with value: 0.45025086851258206 and parameters: {'learning_rate': 0.18900714139879496, 'num_leaves': 286, 'max_depth': 14, 'feature_fraction': 0.7518084901391857, 'bagging_fraction': 0.8837223394432299, 'lambda_l1': 0.15054899322936355, 'lambda_l2': 7.048036655844342}. Best is trial 42 with value: 0.4488002186655424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.450617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:25:41,221] Trial 44 finished with value: 0.4506172862325816 and parameters: {'learning_rate': 0.17729979478731114, 'num_leaves': 262, 'max_depth': 15, 'feature_fraction': 0.8173424781081493, 'bagging_fraction': 0.8076417393117806, 'lambda_l1': 1.3352452056918112, 'lambda_l2': 8.662765573030816}. Best is trial 42 with value: 0.4488002186655424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.45049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:28:43,076] Trial 45 finished with value: 0.45048978544902546 and parameters: {'learning_rate': 0.1930494341351584, 'num_leaves': 285, 'max_depth': 13, 'feature_fraction': 0.8446091590129626, 'bagging_fraction': 0.7832645921770944, 'lambda_l1': 0.7686294969726198, 'lambda_l2': 7.542445878884051}. Best is trial 42 with value: 0.4488002186655424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.49112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:32:57,803] Trial 46 finished with value: 0.4911203777838928 and parameters: {'learning_rate': 0.011627045031191552, 'num_leaves': 275, 'max_depth': 14, 'feature_fraction': 0.8858212939338967, 'bagging_fraction': 0.7176209909751331, 'lambda_l1': 1.6583644091430947, 'lambda_l2': 5.245575532947917}. Best is trial 42 with value: 0.4488002186655424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.467408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:37:36,871] Trial 47 finished with value: 0.46740844918849667 and parameters: {'learning_rate': 0.03490428188053521, 'num_leaves': 292, 'max_depth': 12, 'feature_fraction': 0.8593025200387122, 'bagging_fraction': 0.7443568321356486, 'lambda_l1': 2.292280215959863, 'lambda_l2': 6.3875674432151825}. Best is trial 42 with value: 0.4488002186655424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.482866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:38:56,215] Trial 48 finished with value: 0.48286619461775465 and parameters: {'learning_rate': 0.14761384966812108, 'num_leaves': 257, 'max_depth': 3, 'feature_fraction': 0.7107040637507662, 'bagging_fraction': 0.9064498009286492, 'lambda_l1': 0.5146823304446094, 'lambda_l2': 4.732260719056257}. Best is trial 42 with value: 0.4488002186655424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.474517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:40:42,041] Trial 49 finished with value: 0.47451737357624235 and parameters: {'learning_rate': 0.11856434388374129, 'num_leaves': 268, 'max_depth': 5, 'feature_fraction': 0.7825763803063628, 'bagging_fraction': 0.8735126049131225, 'lambda_l1': 3.825478000466773, 'lambda_l2': 9.02721392655709}. Best is trial 42 with value: 0.4488002186655424.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# LGBM tuning\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbosity\": -1,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 16),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n",
    "    }\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train_transformed, y_train)\n",
    "    lgb_valid = lgb.Dataset(X_val_transformed, y_val, reference=lgb_train)\n",
    "\n",
    "    model = lgb.train(params, lgb_train,\n",
    "                      valid_sets=[lgb_valid],\n",
    "                      callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=0)],\n",
    "                    )\n",
    "    preds = model.predict(X_val_transformed)\n",
    "    return mean_squared_error(y_val, preds) \n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)  # 1 hour\n",
    "\n",
    "# Save the best model to pkl\n",
    "with open(f'{base_path}\\\\lightgbm\\\\lgbm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(study, f)\n",
    "    \n",
    "best_model = lgb.LGBMRegressor(**study.best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af9bf279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for warm user warm item: 0.3818\n",
      "RMSE for warm user warm item: 0.6179\n",
      "MAE for warm user warm item: 0.4561\n",
      "--------------------------------------------------\n",
      "MSE for warm user cold item: 0.4436\n",
      "RMSE for warm user cold item: 0.6660\n",
      "MAE for warm user cold item: 0.5046\n",
      "--------------------------------------------------\n",
      "MSE for cold user warm item: 0.4804\n",
      "RMSE for cold user warm item: 0.6931\n",
      "MAE for cold user warm item: 0.5139\n",
      "--------------------------------------------------\n",
      "MSE for cold user cold item: 0.5864\n",
      "RMSE for cold user cold item: 0.7658\n",
      "MAE for cold user cold item: 0.5860\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test sets\n",
    "best_model.fit(X_train_transformed, y_train, feature_name=list(feature_names))\n",
    "y_pred_uwarm_iwarm = best_model.predict(X_test_uwarm_iwarm_transformed)\n",
    "y_pred_uwarm_icold = best_model.predict(X_test_uwarm_icold_transformed)\n",
    "y_pred_ucold_iwarm = best_model.predict(X_test_ucold_iwarm_transformed)\n",
    "y_pred_ucold_icold = best_model.predict(X_test_ucold_icold_transformed)\n",
    "\n",
    "# MSE\n",
    "mse_uwarm_iwarm = mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mse_uwarm_icold = mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# RMSE\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# MAE\n",
    "mae_uwarm_iwarm = mean_absolute_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# Print the results\n",
    "print(f'MSE for warm user warm item: {mse_uwarm_iwarm:.4f}')\n",
    "print(f'RMSE for warm user warm item: {rmse_uwarm_iwarm:.4f}')\n",
    "print(f'MAE for warm user warm item: {mae_uwarm_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for warm user cold item: {mse_uwarm_icold:.4f}')\n",
    "print(f'RMSE for warm user cold item: {rmse_uwarm_icold:.4f}')\n",
    "print(f'MAE for warm user cold item: {mae_uwarm_icold:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user warm item: {mse_ucold_iwarm:.4f}')\n",
    "print(f'RMSE for cold user warm item: {rmse_ucold_iwarm:.4f}')\n",
    "print(f'MAE for cold user warm item: {mae_ucold_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user cold item: {mse_ucold_icold:.4f}')\n",
    "print(f'RMSE for cold user cold item: {rmse_ucold_icold:.4f}')\n",
    "print(f'MAE for cold user cold item: {mae_ucold_icold:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f386902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 22:57:08,107] A new study created in memory with name: no-name-fc44de7d-bc8e-4cc3-ba9d-0ec1af8e886d\n",
      "[I 2025-05-18 23:01:40,782] Trial 0 finished with value: 0.4635090842655603 and parameters: {'learning_rate': 0.08328078417767078, 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.7592351861881813, 'colsample_bytree': 0.6910308796289071, 'lambda': 8.758232003970122, 'alpha': 1.864305453206434}. Best is trial 0 with value: 0.4635090842655603.\n",
      "[I 2025-05-18 23:09:53,233] Trial 1 finished with value: 0.46107416838745335 and parameters: {'learning_rate': 0.03105084920165796, 'max_depth': 12, 'min_child_weight': 8, 'subsample': 0.6462569000517838, 'colsample_bytree': 0.9288487123658122, 'lambda': 5.961820518044934, 'alpha': 2.557103185263112}. Best is trial 1 with value: 0.46107416838745335.\n",
      "[I 2025-05-18 23:13:59,326] Trial 2 finished with value: 0.46589010614200993 and parameters: {'learning_rate': 0.052321244852632585, 'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.8854229709107755, 'colsample_bytree': 0.6910034361438415, 'lambda': 9.40933681789036, 'alpha': 1.4687795991084018}. Best is trial 1 with value: 0.46107416838745335.\n",
      "[I 2025-05-18 23:16:44,475] Trial 3 finished with value: 0.4817292428516086 and parameters: {'learning_rate': 0.03589885387740283, 'max_depth': 6, 'min_child_weight': 8, 'subsample': 0.9035960821568759, 'colsample_bytree': 0.8334785519230089, 'lambda': 8.962917635394929, 'alpha': 2.8635362516118077}. Best is trial 1 with value: 0.46107416838745335.\n",
      "[I 2025-05-18 23:27:19,151] Trial 4 finished with value: 0.44147346339150023 and parameters: {'learning_rate': 0.062445751734370204, 'max_depth': 16, 'min_child_weight': 4, 'subsample': 0.9966508070319048, 'colsample_bytree': 0.7086686149874889, 'lambda': 8.262112600385063, 'alpha': 8.039872918055984}. Best is trial 4 with value: 0.44147346339150023.\n",
      "[I 2025-05-18 23:30:53,924] Trial 5 finished with value: 0.45664769866846183 and parameters: {'learning_rate': 0.1286099966054766, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.8818786829178731, 'colsample_bytree': 0.6671391623461257, 'lambda': 4.3157720339584955, 'alpha': 0.8361521586554743}. Best is trial 4 with value: 0.44147346339150023.\n",
      "[I 2025-05-18 23:36:53,965] Trial 6 finished with value: 0.4452926089983159 and parameters: {'learning_rate': 0.10076688450069322, 'max_depth': 14, 'min_child_weight': 6, 'subsample': 0.6917985015721109, 'colsample_bytree': 0.8564550313883308, 'lambda': 4.084147435163008, 'alpha': 2.285274187748784}. Best is trial 4 with value: 0.44147346339150023.\n",
      "[I 2025-05-18 23:40:57,282] Trial 7 finished with value: 0.45956851943257 and parameters: {'learning_rate': 0.07578719328313885, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.8238125732849381, 'colsample_bytree': 0.7976999712500996, 'lambda': 6.916848448298523, 'alpha': 2.9422769237823667}. Best is trial 4 with value: 0.44147346339150023.\n",
      "[I 2025-05-18 23:45:16,867] Trial 8 finished with value: 0.4463584252925899 and parameters: {'learning_rate': 0.1711463789637622, 'max_depth': 13, 'min_child_weight': 6, 'subsample': 0.6943362504590839, 'colsample_bytree': 0.9234495820715078, 'lambda': 3.1679538192165904, 'alpha': 5.445109818431831}. Best is trial 4 with value: 0.44147346339150023.\n",
      "[I 2025-05-18 23:50:57,566] Trial 9 finished with value: 0.4659035691097603 and parameters: {'learning_rate': 0.0279093977868025, 'max_depth': 11, 'min_child_weight': 4, 'subsample': 0.8867273941253107, 'colsample_bytree': 0.7411825586484624, 'lambda': 3.3394957193851607, 'alpha': 5.034867941775581}. Best is trial 4 with value: 0.44147346339150023.\n",
      "[I 2025-05-19 00:00:05,171] Trial 10 finished with value: 0.43850399173290255 and parameters: {'learning_rate': 0.1357843253424916, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.9992428470624329, 'colsample_bytree': 0.6177489853447694, 'lambda': 1.7100192403923673, 'alpha': 9.161818171120025}. Best is trial 10 with value: 0.43850399173290255.\n",
      "[I 2025-05-19 00:09:31,273] Trial 11 finished with value: 0.4380871026434469 and parameters: {'learning_rate': 0.14084149339152463, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.9887106099744349, 'colsample_bytree': 0.6221236197416847, 'lambda': 1.95343329379311, 'alpha': 9.189895834912342}. Best is trial 11 with value: 0.4380871026434469.\n",
      "[I 2025-05-19 00:19:12,105] Trial 12 finished with value: 0.43789747826031783 and parameters: {'learning_rate': 0.14693008543851838, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.9997322720739797, 'colsample_bytree': 0.610633586719646, 'lambda': 0.9996117654665304, 'alpha': 9.338093308809437}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 00:21:11,339] Trial 13 finished with value: 0.4817211969721539 and parameters: {'learning_rate': 0.17785700986377848, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.9430631536783766, 'colsample_bytree': 0.6005417993408781, 'lambda': 0.12546063040968747, 'alpha': 9.944027608755126}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 00:29:14,430] Trial 14 finished with value: 0.44077362327895203 and parameters: {'learning_rate': 0.14290487186930326, 'max_depth': 15, 'min_child_weight': 3, 'subsample': 0.9474101562740453, 'colsample_bytree': 0.6357561542594998, 'lambda': 0.17018641619988006, 'alpha': 7.379695983832848}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 00:35:40,506] Trial 15 finished with value: 0.44174650368983925 and parameters: {'learning_rate': 0.19874209675660823, 'max_depth': 14, 'min_child_weight': 1, 'subsample': 0.9496993052745042, 'colsample_bytree': 0.9871746734877067, 'lambda': 1.6629796115266122, 'alpha': 6.876098715673484}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 00:37:44,788] Trial 16 finished with value: 0.4662705192086903 and parameters: {'learning_rate': 0.1536707581715084, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.8090107116892042, 'colsample_bytree': 0.7640397163890981, 'lambda': 1.6207051165753243, 'alpha': 8.491096976205892}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 00:44:04,605] Trial 17 finished with value: 0.438511760891971 and parameters: {'learning_rate': 0.12143043440151682, 'max_depth': 16, 'min_child_weight': 2, 'subsample': 0.8448715518050766, 'colsample_bytree': 0.6493575574073466, 'lambda': 2.469645370119864, 'alpha': 6.3277484544131175}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 00:48:09,150] Trial 18 finished with value: 0.44267644872429585 and parameters: {'learning_rate': 0.1598822722120082, 'max_depth': 14, 'min_child_weight': 5, 'subsample': 0.7634887345497099, 'colsample_bytree': 0.7420179709768153, 'lambda': 0.9357236276438734, 'alpha': 4.133353282612013}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 00:52:00,195] Trial 19 finished with value: 0.4503509626834632 and parameters: {'learning_rate': 0.10342288160241823, 'max_depth': 12, 'min_child_weight': 2, 'subsample': 0.9662397483468129, 'colsample_bytree': 0.6000871808284092, 'lambda': 5.111593023361671, 'alpha': 9.928986660955553}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 00:53:57,504] Trial 20 finished with value: 0.4652242947710339 and parameters: {'learning_rate': 0.18503337712611473, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.9153397818183562, 'colsample_bytree': 0.6547182282052172, 'lambda': 2.651212354414574, 'alpha': 8.508912798518814}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:00:00,448] Trial 21 finished with value: 0.43856291138944636 and parameters: {'learning_rate': 0.1350141394966187, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.9995036313191974, 'colsample_bytree': 0.6279730178385923, 'lambda': 1.5375437257555515, 'alpha': 9.206778669541354}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:05:25,325] Trial 22 finished with value: 0.4406441877801523 and parameters: {'learning_rate': 0.12137552157427586, 'max_depth': 15, 'min_child_weight': 1, 'subsample': 0.9804188621968293, 'colsample_bytree': 0.6117336086139666, 'lambda': 0.9471233730947124, 'alpha': 9.13587016073013}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:10:20,448] Trial 23 finished with value: 0.43938442534784056 and parameters: {'learning_rate': 0.15409277573459937, 'max_depth': 15, 'min_child_weight': 2, 'subsample': 0.9228145792905528, 'colsample_bytree': 0.6715767602696996, 'lambda': 2.398395130776805, 'alpha': 8.04823809246776}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:14:18,828] Trial 24 finished with value: 0.4473114444731158 and parameters: {'learning_rate': 0.11420027638213057, 'max_depth': 13, 'min_child_weight': 1, 'subsample': 0.970906270617564, 'colsample_bytree': 0.7030142835379449, 'lambda': 0.7431814539398692, 'alpha': 6.21268618659277}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:20:15,136] Trial 25 finished with value: 0.4387899100277363 and parameters: {'learning_rate': 0.13922848597138857, 'max_depth': 16, 'min_child_weight': 2, 'subsample': 0.849295301681222, 'colsample_bytree': 0.631350161799099, 'lambda': 3.8509182265187776, 'alpha': 7.4904188943293954}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:23:51,391] Trial 26 finished with value: 0.445636785491953 and parameters: {'learning_rate': 0.16586122156122735, 'max_depth': 13, 'min_child_weight': 3, 'subsample': 0.9989733652606417, 'colsample_bytree': 0.7300258721080966, 'lambda': 2.06949471996987, 'alpha': 9.170169567434815}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:29:20,948] Trial 27 finished with value: 0.44311322536528686 and parameters: {'learning_rate': 0.09027756138488799, 'max_depth': 15, 'min_child_weight': 10, 'subsample': 0.6037404548468053, 'colsample_bytree': 0.7715479700084089, 'lambda': 5.287209786167207, 'alpha': 4.107777284929691}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:30:41,019] Trial 28 finished with value: 0.4830689926542036 and parameters: {'learning_rate': 0.14566374262330858, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.9333190105840251, 'colsample_bytree': 0.6681047254437886, 'lambda': 3.1401064282690907, 'alpha': 9.272001269234545}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:34:17,019] Trial 29 finished with value: 0.4513450684495619 and parameters: {'learning_rate': 0.11214800821134868, 'max_depth': 12, 'min_child_weight': 1, 'subsample': 0.7658681389163503, 'colsample_bytree': 0.6798254555813928, 'lambda': 1.2801639218124974, 'alpha': 7.320338399357009}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:38:23,061] Trial 30 finished with value: 0.44151954120793674 and parameters: {'learning_rate': 0.18682811252939435, 'max_depth': 14, 'min_child_weight': 2, 'subsample': 0.9677657240191297, 'colsample_bytree': 0.6299743654781467, 'lambda': 0.012670679743273627, 'alpha': 8.529717251454064}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:44:35,071] Trial 31 finished with value: 0.4388146436629729 and parameters: {'learning_rate': 0.12664422849867135, 'max_depth': 16, 'min_child_weight': 2, 'subsample': 0.7186107627347629, 'colsample_bytree': 0.6434655470617723, 'lambda': 2.3856607319221004, 'alpha': 6.364373981207521}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:51:56,644] Trial 32 finished with value: 0.43934428308375184 and parameters: {'learning_rate': 0.09068752380322125, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.859487752946813, 'colsample_bytree': 0.6536434750611474, 'lambda': 2.853234943149463, 'alpha': 6.354626050315094}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 01:56:58,247] Trial 33 finished with value: 0.43975385087992525 and parameters: {'learning_rate': 0.14773131873731818, 'max_depth': 15, 'min_child_weight': 2, 'subsample': 0.9648278606152476, 'colsample_bytree': 0.613821932563405, 'lambda': 2.0015652255220973, 'alpha': 9.933993799253898}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:02:57,231] Trial 34 finished with value: 0.4392711887788392 and parameters: {'learning_rate': 0.11966874576745115, 'max_depth': 16, 'min_child_weight': 3, 'subsample': 0.852473964970794, 'colsample_bytree': 0.7065545589138504, 'lambda': 0.7127681707593757, 'alpha': 8.306653591928919}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:07:12,415] Trial 35 finished with value: 0.44495279656430986 and parameters: {'learning_rate': 0.13129488087443866, 'max_depth': 14, 'min_child_weight': 1, 'subsample': 0.9004667233271408, 'colsample_bytree': 0.864515422524192, 'lambda': 6.992172958851645, 'alpha': 5.698655769919546}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:08:49,695] Trial 36 finished with value: 0.4732649462861561 and parameters: {'learning_rate': 0.16013739604663088, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.990293587460144, 'colsample_bytree': 0.6878453742143984, 'lambda': 4.454889051387628, 'alpha': 7.687525577158618}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:15:28,009] Trial 37 finished with value: 0.44225465662414437 and parameters: {'learning_rate': 0.07528042850622563, 'max_depth': 15, 'min_child_weight': 7, 'subsample': 0.9265421282350467, 'colsample_bytree': 0.6217867950464612, 'lambda': 1.9603029103644276, 'alpha': 6.874205051161725}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:18:48,042] Trial 38 finished with value: 0.4537446914192879 and parameters: {'learning_rate': 0.10825286389717305, 'max_depth': 11, 'min_child_weight': 2, 'subsample': 0.8254238451988867, 'colsample_bytree': 0.6460801173928497, 'lambda': 3.5797825356208355, 'alpha': 0.015311736644409102}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:23:13,801] Trial 39 finished with value: 0.4472229205173356 and parameters: {'learning_rate': 0.09661951055752886, 'max_depth': 13, 'min_child_weight': 4, 'subsample': 0.782355456205979, 'colsample_bytree': 0.6552921642334162, 'lambda': 1.2738667548098759, 'alpha': 3.672526828776339}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:33:15,170] Trial 40 finished with value: 0.47007970109665087 and parameters: {'learning_rate': 0.012092852698410289, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.7289147003307932, 'colsample_bytree': 0.7150624640714485, 'lambda': 9.848571407076399, 'alpha': 8.776705973584281}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:39:20,397] Trial 41 finished with value: 0.4382400595341833 and parameters: {'learning_rate': 0.13447112311599532, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.9960127802053373, 'colsample_bytree': 0.6187668755486115, 'lambda': 1.5859924452645169, 'alpha': 9.420857875400019}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:44:33,744] Trial 42 finished with value: 0.440395253840449 and parameters: {'learning_rate': 0.132307852679711, 'max_depth': 15, 'min_child_weight': 2, 'subsample': 0.980971663043293, 'colsample_bytree': 0.6069645597154643, 'lambda': 0.49393227056505173, 'alpha': 9.562960986273414}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:50:59,335] Trial 43 finished with value: 0.4382739176838896 and parameters: {'learning_rate': 0.12435354207785038, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.9482415411046131, 'colsample_bytree': 0.623823238269617, 'lambda': 2.565805167571236, 'alpha': 8.852063107556127}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:55:23,058] Trial 44 finished with value: 0.4438282172618352 and parameters: {'learning_rate': 0.1418100122187692, 'max_depth': 14, 'min_child_weight': 1, 'subsample': 0.9538977904713316, 'colsample_bytree': 0.619893697619384, 'lambda': 1.3446698488788484, 'alpha': 9.60800601516623}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 02:57:51,037] Trial 45 finished with value: 0.45926092883021147 and parameters: {'learning_rate': 0.15071814262778216, 'max_depth': 9, 'min_child_weight': 8, 'subsample': 0.9824681582090525, 'colsample_bytree': 0.6825946463459964, 'lambda': 2.8622527389911028, 'alpha': 8.944503638489095}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 03:03:08,216] Trial 46 finished with value: 0.4386616010944225 and parameters: {'learning_rate': 0.16810788180543335, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.9425058376555981, 'colsample_bytree': 0.892690518312397, 'lambda': 1.9467501506045028, 'alpha': 8.013455924703106}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 03:07:29,237] Trial 47 finished with value: 0.4441436893490654 and parameters: {'learning_rate': 0.12579192579226794, 'max_depth': 14, 'min_child_weight': 3, 'subsample': 0.901342704444501, 'colsample_bytree': 0.8157170865381963, 'lambda': 6.102008461096325, 'alpha': 9.536259056788868}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 03:12:27,092] Trial 48 finished with value: 0.44124551542735574 and parameters: {'learning_rate': 0.13754866364473578, 'max_depth': 15, 'min_child_weight': 2, 'subsample': 0.9567232591096411, 'colsample_bytree': 0.6639498536810293, 'lambda': 8.081613224053598, 'alpha': 7.952800711863776}. Best is trial 12 with value: 0.43789747826031783.\n",
      "[I 2025-05-19 03:17:09,337] Trial 49 finished with value: 0.4397149322484355 and parameters: {'learning_rate': 0.17549729722514432, 'max_depth': 15, 'min_child_weight': 7, 'subsample': 0.9979949764468375, 'colsample_bytree': 0.6020880221242517, 'lambda': 0.49617628786641, 'alpha': 8.82793469841467}. Best is trial 12 with value: 0.43789747826031783.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for XGBoost\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"seed\": 42,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 16),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0.0, 10.0),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0, 10.0),\n",
    "    }\n",
    "\n",
    "    xgb_train = xgb.DMatrix(X_train_transformed, label=y_train, feature_names=list(feature_names))\n",
    "    xgb_valid = xgb.DMatrix(X_val_transformed, label=y_val, feature_names=list(feature_names))\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        xgb_train,\n",
    "        num_boost_round=100,\n",
    "        evals=[(xgb_valid, \"validation\")],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    preds = model.predict(xgb_valid)\n",
    "    return mean_squared_error(y_val, preds)\n",
    "study_xgb = optuna.create_study(direction=\"minimize\")\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "# Save the best model to pkl\n",
    "with open(f'{base_path}\\\\xgboost\\\\xgboost_model.pkl', 'wb') as f:\n",
    "    pickle.dump(study_xgb, f)\n",
    "best_model_xgb = xgb.XGBRegressor(**study_xgb.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f73b955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Denis\\envs\\webmining\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for warm user warm item: 0.3818\n",
      "RMSE for warm user warm item: 0.6179\n",
      "MAE for warm user warm item: 0.4561\n",
      "--------------------------------------------------\n",
      "MSE for warm user cold item: 0.4436\n",
      "RMSE for warm user cold item: 0.6660\n",
      "MAE for warm user cold item: 0.5046\n",
      "--------------------------------------------------\n",
      "MSE for cold user warm item: 0.4804\n",
      "RMSE for cold user warm item: 0.6931\n",
      "MAE for cold user warm item: 0.5139\n",
      "--------------------------------------------------\n",
      "MSE for cold user cold item: 0.5864\n",
      "RMSE for cold user cold item: 0.7658\n",
      "MAE for cold user cold item: 0.5860\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test sets\n",
    "best_model.fit(X_train_transformed, y_train, feature_name=list(feature_names))\n",
    "y_pred_uwarm_iwarm = best_model.predict(X_test_uwarm_iwarm_transformed)\n",
    "y_pred_uwarm_icold = best_model.predict(X_test_uwarm_icold_transformed)\n",
    "y_pred_ucold_iwarm = best_model.predict(X_test_ucold_iwarm_transformed)\n",
    "y_pred_ucold_icold = best_model.predict(X_test_ucold_icold_transformed)\n",
    "\n",
    "# MSE\n",
    "mse_uwarm_iwarm = mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mse_uwarm_icold = mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# RMSE\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# MAE\n",
    "mae_uwarm_iwarm = mean_absolute_error(y_test_uwarm_iwarm, y_pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(y_test_uwarm_icold, y_pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(y_test_ucold_iwarm, y_pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(y_test_ucold_icold, y_pred_ucold_icold)\n",
    "# Print the results\n",
    "print(f'MSE for warm user warm item: {mse_uwarm_iwarm:.4f}')\n",
    "print(f'RMSE for warm user warm item: {rmse_uwarm_iwarm:.4f}')\n",
    "print(f'MAE for warm user warm item: {mae_uwarm_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for warm user cold item: {mse_uwarm_icold:.4f}')\n",
    "print(f'RMSE for warm user cold item: {rmse_uwarm_icold:.4f}')\n",
    "print(f'MAE for warm user cold item: {mae_uwarm_icold:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user warm item: {mse_ucold_iwarm:.4f}')\n",
    "print(f'RMSE for cold user warm item: {rmse_ucold_iwarm:.4f}')\n",
    "print(f'MAE for cold user warm item: {mae_ucold_iwarm:.4f}')\n",
    "print('-' * 50)\n",
    "print(f'MSE for cold user cold item: {mse_ucold_icold:.4f}')\n",
    "print(f'RMSE for cold user cold item: {rmse_ucold_icold:.4f}')\n",
    "print(f'MAE for cold user cold item: {mae_ucold_icold:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
