{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from category_encoders import TargetEncoder, HashingEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import hstack, csr_matrix, issparse, lil_matrix, save_npz, load_npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter of string lists into Python lists\n",
    "# (e.g. \"['a', 'b', 'c']\" â†’ [a, b, c])\n",
    "def parse_list_col(s):\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Converter of 'N.V.' to 0, so column is numeric\n",
    "def parse_vintage(s):\n",
    "    return 0 if s == 'N.V.' else int(s)\n",
    "\n",
    "\n",
    "base_path = '..\\..\\..\\..\\data\\main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a935f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the train and test splits\n",
    "\n",
    "wines = pd.read_csv(\n",
    "    f'{base_path}\\\\XWines_Full_100K_wines.csv', \n",
    "    usecols=['WineID', 'Type', 'Elaborate', 'ABV', 'Body', 'Acidity', 'RegionName', 'WineryName', 'Grapes','Harmonize','Country'],\n",
    "    converters={\n",
    "        'Grapes':    parse_list_col,\n",
    "        'Harmonize': parse_list_col\n",
    "    }\n",
    ")\n",
    "train = pd.read_csv(\n",
    "    f'{base_path}\\\\trainset.csv', \n",
    "    usecols=['UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_uwarm_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_warm_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_uwarm_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_warm_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_ucold_iwarm = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_cold_user_warm_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")\n",
    "test_ucold_icold = pd.read_csv(\n",
    "    f'{base_path}\\\\testset_cold_user_cold_item.csv', \n",
    "    usecols=['RatingID', 'UserID', 'WineID', 'Rating', 'Date', 'Vintage'],\n",
    "    parse_dates=['Date'],\n",
    "    date_format=lambda s: pd.to_datetime(s),\n",
    "    converters={'Vintage': parse_vintage}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c139bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ratings with wines metadata on 'WineID'\n",
    "train = train.merge(wines, on='WineID', how='left')\n",
    "test_uwarm_iwarm = test_uwarm_iwarm.merge(wines, on='WineID', how='left')\n",
    "test_uwarm_icold = test_uwarm_icold.merge(wines, on='WineID', how='left')\n",
    "test_ucold_iwarm = test_ucold_iwarm.merge(wines, on='WineID', how='left')\n",
    "test_ucold_icold = test_ucold_icold.merge(wines, on='WineID', how='left')\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test warm user warm item shape: {test_uwarm_iwarm.shape}\")\n",
    "print(f\"Test warm user cold item shape: {test_uwarm_icold.shape}\")\n",
    "print(f\"Test cold user warm item shape: {test_ucold_iwarm.shape}\")\n",
    "print(f\"Test cold user cold item shape: {test_ucold_icold.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe018c1",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing\n",
    "### Preprocessing methods for different features:\n",
    "* **Standard Scaler** - is used for numerical type columns\n",
    "    * **ABV**\n",
    "    * **Vintage** (formatted to be numerical)\n",
    "    * **DaysAgo(Date)** - see below\n",
    "* **One-hot-encoding** - is used for categorical features, but is limited by the number of categories within a feature:\n",
    "    * **Type**\n",
    "    * **Body**\n",
    "    * **Acidity**\n",
    "    * **Elaborate**\n",
    "* **Multi-Label** - is used for categorical features with too many categories, where also multiple active categories could be possible:\n",
    "    * **Grapes** (774 classes)\n",
    "    * **Harmonize** (~64 classes)\n",
    "* **Target Encoding** - used for text features and user IDs, wine IDs. Used with KFold(5 folds) to prevent data leakage, i.e. encoded feature never knows about it's own target value\n",
    "* **Date encoding** - created custom object to convert datetime column to DaysAgo from the most recent record column. This way we keep information about time-related information and reduce feature to be simply numerical. **Standard Scaler** applied afterwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate features\n",
    "\n",
    "# Use StandardScaler for numerical features (create binary Is_NonVintage column derived from Vintage and maybe fill NaN values with 0)\n",
    "numerical_features = ['ABV', 'Vintage']\n",
    "# Use one-hot encoding for categorical features\n",
    "categorical_features = ['Type', 'Elaborate', 'Body', 'Acidity']\n",
    "# Use multilabel binarizer for multilabel features\n",
    "multilabel_features = ['Grapes', 'Harmonize']\n",
    "# Use target encoding for Country\n",
    "targetencoder_features = ['Country']\n",
    "# Use frequency encoder for IDs and high cardinality features\n",
    "frequency_features = ['WineID', 'UserID', 'WineryName', 'RegionName']\n",
    "# Use conversion to DaysAgo for time-based features\n",
    "date_features = ['Date']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37046423",
   "metadata": {},
   "source": [
    "\n",
    "* **Create Preprocessing pipeline**:\n",
    "    ##### **Important**: Since during Grapes column encoding we create 774 classes + there are around 100 classes from other encoders, the pandas DataFrame would require too much RAM (I recieved 69GB memory allocation error only for the Grapes column) and same happening for the dense array (numpy), I used csr_matrix from scipy.sparse and some additional optimizations for MultiLabelBinarizer in particular, to be able to store all the preprocessed features. More info in code below.\n",
    "\n",
    "    * **Created custom object for Date column preprocessing**\n",
    "    * **Created Wrappers for other preprocessors to always return sparse csr matricies**. For OneHotEncoding there is already implemented sparse output. For StandardScaler in date_pipeline wrapper is not required, since the input is already a csr matrix.\n",
    "    * **Created pipelines for each preprocessor and a general pipeline to combine everything together, using ColumnTransformer** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574718d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Date preprocessor\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transforms a single datetime column into 'days ago' relative to the latest date in training data.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Expect a DataFrame with a single datetime column\n",
    "        self.col = X.columns[0]\n",
    "        self.column = f\"{self.col}_days_ago\"\n",
    "        self.reference_date = pd.to_datetime(X[self.col]).max()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        days_ago = (self.reference_date - pd.to_datetime(X[self.col])).dt.days\n",
    "        \n",
    "        return csr_matrix(days_ago.values.reshape(-1, 1))\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array([self.column])\n",
    "    \n",
    "\n",
    "class MultiLabelWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = []\n",
    "        for col in X.columns:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            # Fill missing with empty list for consistent fitting\n",
    "            safe_col = X[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "            mlb.fit(safe_col)\n",
    "            self.encoders[col] = mlb\n",
    "            self.feature_names.extend([f\"{col}__{cls}\" for cls in mlb.classes_])\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        matricies = []\n",
    "        for col in X.columns:\n",
    "            mlb = self.encoders[col]\n",
    "            class_index = {cls: i for i, cls in enumerate(mlb.classes_)}\n",
    "            n_rows = len(X)\n",
    "            n_classes = len(mlb.classes_)\n",
    "            sparse = lil_matrix((n_rows, n_classes), dtype=np.uint8)\n",
    "\n",
    "            for i, labels in enumerate(X[col]):\n",
    "                # Handle missing or malformed entries\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = []\n",
    "                for label in labels:\n",
    "                    if label in class_index:\n",
    "                        sparse[i, class_index[label]] = 1\n",
    "\n",
    "            matricies.append(sparse.tocsr())\n",
    "        return hstack(matricies, format='csr')\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "\n",
    "class TargetEncoderWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.feature_names = []\n",
    "        self.global_means = {}\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        for col in X.columns:\n",
    "            te = TargetEncoder(cols=[col])\n",
    "            te.fit(X[[col]], y, cv=kf)\n",
    "            self.encoders[col] = te\n",
    "            self.global_means[col] = y.mean() \n",
    "            self.feature_names.append(f'{col}_target_encoded')\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        matricies = []\n",
    "        for col in X.columns:\n",
    "            te = self.encoders[col]\n",
    "            df_encoded = te.transform(X[[col]])\n",
    "            arr = df_encoded[col].fillna(self.global_means[col]).values.reshape(-1, 1)\n",
    "            matricies.append(csr_matrix(arr))\n",
    "        return hstack(matricies, format='csr')\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "    \n",
    "    \n",
    "class FrequencyEncoderWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.freq_maps = {}\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.freq_maps = {}\n",
    "        self.feature_names = [f\"{col}_freq\" for col in X.columns]\n",
    "        for col in X.columns:\n",
    "            self.freq_maps[col] = X[col].value_counts(normalize=True).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        matrices = []\n",
    "        for col in X.columns:\n",
    "            freq = X[col].map(self.freq_maps[col]).fillna(0).values.reshape(-1, 1)\n",
    "            matrices.append(csr_matrix(freq))\n",
    "        return hstack(matrices, format='csr')\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n",
    "\n",
    "\n",
    "class StandardScalerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler(with_mean=False)\n",
    "        self.feature_names = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        # Always return sparse csr_matrix\n",
    "        if not issparse(X_scaled):\n",
    "            X_scaled = csr_matrix(X_scaled)\n",
    "        return X_scaled\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Preprocessing pipeline\n",
    "\n",
    "# Numerical\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('scaler', StandardScalerWrapper()),\n",
    "    ])\n",
    "\n",
    "# Categorical via one-hot-encoding\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "\n",
    "# Categorical via MultiLabelBinarizer\n",
    "multilabel_pipeline = Pipeline([\n",
    "    ('multilabel', MultiLabelWrapper())\n",
    "])\n",
    "\n",
    "# Country via target encoding\n",
    "target_pipeline = Pipeline([\n",
    "    ('target', TargetEncoderWrapper())\n",
    "])\n",
    "\n",
    "# High cardinality categorical features via frequency encoding\n",
    "# (i.e. WineryName, RegionName, UserID, WineID)\n",
    "frequency_pipeline = Pipeline([\n",
    "    ('frequency', FrequencyEncoderWrapper())\n",
    "])\n",
    "\n",
    "# Datetime via custom date transformer\n",
    "date_pipeline = Pipeline([\n",
    "    ('date', DateTransformer()),\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "# Preprocessor\n",
    "# Remainnder contains RatingID column, which is not needed for training neither for testing\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numerical', numerical_pipeline, numerical_features),\n",
    "    ('categorical', categorical_pipeline, categorical_features),\n",
    "    ('multilabel', multilabel_pipeline, multilabel_features),\n",
    "    ('target', target_pipeline, targetencoder_features),\n",
    "    ('frequency', frequency_pipeline, frequency_features),\n",
    "    ('date', date_pipeline, date_features)\n",
    "], remainder='drop')\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop target column for all datasets\n",
    "\n",
    "X_train = train.drop(columns=['Rating'])\n",
    "y_train = train['Rating']\n",
    "\n",
    "X_test_uwarm_iwarm = test_uwarm_iwarm.drop(columns=['Rating'])\n",
    "y_test_uwarm_iwarm = test_uwarm_iwarm['Rating']\n",
    "\n",
    "X_test_uwarm_icold = test_uwarm_icold.drop(columns=['Rating'])\n",
    "y_test_uwarm_icold = test_uwarm_icold['Rating']\n",
    "\n",
    "X_test_ucold_iwarm = test_ucold_iwarm.drop(columns=['Rating'])\n",
    "y_test_ucold_iwarm = test_ucold_iwarm['Rating']\n",
    "\n",
    "X_test_ucold_icold = test_ucold_icold.drop(columns=['Rating'])\n",
    "y_test_ucold_icold = test_ucold_icold['Rating']\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/val split for hyperparameter tuning\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222cb59",
   "metadata": {},
   "source": [
    "\n",
    "* **Fit preprocessing pipeline on training data. We pass target variable there for the Target Encoder**\n",
    "* **Transform train and test sets on a fitted preprocessor**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the preprocessing pipeline\n",
    "preprocessing_pipeline.fit(X_train, y_train)\n",
    "\n",
    "X_train_transformed = preprocessing_pipeline.transform(X_train)\n",
    "X_val_transformed = preprocessing_pipeline.transform(X_val)\n",
    "X_test_uwarm_iwarm_transformed = preprocessing_pipeline.transform(X_test_uwarm_iwarm)\n",
    "X_test_uwarm_icold_transformed = preprocessing_pipeline.transform(X_test_uwarm_icold)\n",
    "X_test_ucold_iwarm_transformed = preprocessing_pipeline.transform(X_test_ucold_iwarm)\n",
    "X_test_ucold_icold_transformed = preprocessing_pipeline.transform(X_test_ucold_icold)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = preprocessing_pipeline.get_feature_names_out()\n",
    "# Check the size of feature names and transformed data features \n",
    "print(f\"Feature names size: {len(feature_names)}\")\n",
    "print(f\"Transformed train data size: {X_train_transformed.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba84505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save transformed data to npz\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_train_transformed.npz', X_train_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_val_transformed.npz', X_val_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_test_uwarm_iwarm_transformed.npz', X_test_uwarm_iwarm_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_test_uwarm_icold_transformed.npz', X_test_uwarm_icold_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_test_ucold_iwarm_transformed.npz', X_test_ucold_iwarm_transformed)\n",
    "save_npz(f'{base_path}\\\\preprocessed\\\\X_test_ucold_icold_transformed.npz', X_test_ucold_icold_transformed)\n",
    "# Save target values to csv\n",
    "y_train.to_csv(f'{base_path}\\\\preprocessed\\\\y_train.csv', index=False)\n",
    "y_val.to_csv(f'{base_path}\\\\preprocessed\\\\y_val.csv', index=False)\n",
    "y_test_uwarm_iwarm.to_csv(f'{base_path}\\\\preprocessed\\\\y_test_uwarm_iwarm.csv', index=False)\n",
    "y_test_uwarm_icold.to_csv(f'{base_path}\\\\preprocessed\\\\y_test_uwarm_icold.csv', index=False)\n",
    "y_test_ucold_iwarm.to_csv(f'{base_path}\\\\preprocessed\\\\y_test_ucold_iwarm.csv', index=False)\n",
    "y_test_ucold_icold.to_csv(f'{base_path}\\\\preprocessed\\\\y_test_ucold_icold.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
