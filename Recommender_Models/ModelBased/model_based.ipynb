{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96787365",
   "metadata": {},
   "source": [
    "# Model-Based recommender system\n",
    "## Plan\n",
    "* Dataset of WineID,UserID,Rating\n",
    "* Train using MF(SVD)\n",
    "* Evaluate\n",
    "\n",
    "### Singular Value Decomposition (SVD) algorithm\n",
    "#### https://surprise.readthedocs.io/en/stable/matrix_factorization.html\n",
    "Is a Matrix Factorization algorithm (Probabalistic MF if biases are not used). Minimizes the regularized square error by straightforward Stohastic Gradient Descent.\n",
    "\n",
    "Pros:\n",
    "- Has high accuracy\n",
    "- Scales well on large data\n",
    "- Dimestionality reduction\n",
    "\n",
    "Cons:\n",
    "- Doesn't handle cold-start problem without additional modifications (like hybrid models, pre-training clustering together similar users and items)\n",
    "- Hard to interpret due to latent factors\n",
    "- SVD doesn't use any metadata\n",
    "\n",
    "Why use SVD?\n",
    "1. Large dataset (trainset ~16 mill rows)\n",
    "2. No user metadata\n",
    "3. Handles only warm-warm start, is not designed to handle any of cold-start scenarions.\n",
    "However the RMSE scores look good here since in cold start cases the SVD model from surprise just take the global mean for unseen items/users.\n",
    "Attempt to handle cold-start with Hybrid model (CF + Content-based)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a81cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from surprise import SVD, Dataset, Reader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09271dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "base_path = '..\\..\\..\\data\\main'\n",
    "\n",
    "train = pd.read_csv(f'{base_path}\\\\trainset.csv', usecols=['UserID', 'WineID', 'Rating'])\n",
    "test_uwarm_iwarm = pd.read_csv(f'{base_path}\\\\testset_warm_user_warm_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_uwarm_icold = pd.read_csv(f'{base_path}\\\\testset_warm_user_cold_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_ucold_iwarm = pd.read_csv(f'{base_path}\\\\testset_cold_user_warm_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])\n",
    "test_ucold_icold = pd.read_csv(f'{base_path}\\\\testset_cold_user_cold_item.csv', usecols=['RatingID', 'UserID', 'WineID', 'Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c551542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (16917894, 3)\n",
      "Test set shape (warm user, warm item): (2036778, 4)\n",
      "Test set shape (warm user, cold item): (35456, 4)\n",
      "Test set shape (cold user, warm item): (506800, 4)\n",
      "Test set shape (cold user, cold item): (16504, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Train set shape:', train.shape)\n",
    "print('Test set shape (warm user, warm item):', test_uwarm_iwarm.shape)\n",
    "print('Test set shape (warm user, cold item):', test_uwarm_icold.shape)\n",
    "print('Test set shape (cold user, warm item):', test_ucold_iwarm.shape)\n",
    "print('Test set shape (cold user, cold item):', test_ucold_icold.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdded934",
   "metadata": {},
   "source": [
    "## SVD using Surprise library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec1e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use only raw data since Surprise handles the encoding internally\n",
    "\n",
    "# For train: convert to Surprise format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "train_data = Dataset.load_from_df(train[['UserID', 'WineID', 'Rating']], reader)\n",
    "train_surprise = train_data.build_full_trainset()\n",
    "\n",
    "# For test: convert to Surprise format\n",
    "test_uwarm_iwarm_surprise = list(test_uwarm_iwarm[['UserID', 'WineID', 'Rating']].itertuples(index=False, name=None))\n",
    "test_uwarm_icold_surprise = list(test_uwarm_icold[['UserID', 'WineID', 'Rating']].itertuples(index=False, name=None))\n",
    "test_ucold_iwarm_surprise = list(test_ucold_iwarm[['UserID', 'WineID', 'Rating']].itertuples(index=False, name=None))\n",
    "test_ucold_icold_surprise = list(test_ucold_icold[['UserID', 'WineID', 'Rating']].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3186b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Results:\n",
      "MSE (warm user, warm item): 0.3198472927282139\n",
      "RMSE (warm user, warm item): 0.5655504334082098\n",
      "MAE (warm user, warm item): 0.412036745658953\n",
      "--------------------------------------------------\n",
      "MSE (warm user, cold item): 0.41533766974870867\n",
      "RMSE (warm user, cold item): 0.644466965599253\n",
      "MAE (warm user, cold item): 0.47336789039746074\n",
      "--------------------------------------------------\n",
      "MSE (cold user, warm item): 0.44334311057386555\n",
      "RMSE (cold user, warm item): 0.6658401539212437\n",
      "MAE (cold user, warm item): 0.49323154327054913\n",
      "--------------------------------------------------\n",
      "MSE (cold user, cold item): 0.5983717312186697\n",
      "RMSE (cold user, cold item): 0.7735449122182045\n",
      "MAE (cold user, cold item): 0.574148821209833\n"
     ]
    }
   ],
   "source": [
    "# Train SVD\n",
    "model = SVD()\n",
    "model.fit(train_surprise)\n",
    "\n",
    "# Predict ratings for test sets\n",
    "pred_uwarm_iwarm = model.test(test_uwarm_iwarm_surprise)\n",
    "pred_uwarm_icold = model.test(test_uwarm_icold_surprise)\n",
    "pred_ucold_iwarm = model.test(test_ucold_iwarm_surprise)\n",
    "pred_ucold_icold = model.test(test_ucold_icold_surprise)\n",
    "\n",
    "# Convert redicted ratings to list\n",
    "pred_uwarm_iwarm = [pred.est for pred in pred_uwarm_iwarm]\n",
    "pred_uwarm_icold = [pred.est for pred in pred_uwarm_icold]\n",
    "pred_ucold_iwarm = [pred.est for pred in pred_ucold_iwarm]\n",
    "pred_ucold_icold = [pred.est for pred in pred_ucold_icold]\n",
    "\n",
    "# Save predictions to dataframe \n",
    "test_uwarm_iwarm['Prediction'] = pred_uwarm_iwarm\n",
    "test_uwarm_icold['Prediction'] = pred_uwarm_icold\n",
    "test_ucold_iwarm['Prediction'] = pred_ucold_iwarm\n",
    "test_ucold_icold['Prediction'] = pred_ucold_icold\n",
    "\n",
    "# Write predictions to CSV [RatingID, Prediction]\n",
    "test_uwarm_iwarm.to_csv(f'{base_path}\\\\svd\\\\svd_warm_user_warm_item.csv', index=False, columns=['RatingID', 'Prediction'], header=['RatingID', 'Rating'])\n",
    "test_uwarm_icold.to_csv(f'{base_path}\\\\svd\\\\svd_warm_user_cold_item.csv', index=False, columns=['RatingID', 'Prediction'], header=['RatingID', 'Rating'])\n",
    "test_ucold_iwarm.to_csv(f'{base_path}\\\\svd\\\\svd_cold_user_warm_item.csv', index=False, columns=['RatingID', 'Prediction'], header=['RatingID', 'Rating'])\n",
    "test_ucold_icold.to_csv(f'{base_path}\\\\svd\\\\svd_cold_user_cold_item.csv', index=False, columns=['RatingID', 'Prediction'], header=['RatingID', 'Rating'])\n",
    "\n",
    "# Evaluate MSE\n",
    "mse_uwarm_iwarm = mean_squared_error(test_uwarm_iwarm['Rating'], pred_uwarm_iwarm)\n",
    "mse_uwarm_icold = mean_squared_error(test_uwarm_icold['Rating'], pred_uwarm_icold)\n",
    "mse_ucold_iwarm = mean_squared_error(test_ucold_iwarm['Rating'], pred_ucold_iwarm)\n",
    "mse_ucold_icold = mean_squared_error(test_ucold_icold['Rating'], pred_ucold_icold)\n",
    "\n",
    "# Evaluate RMSE\n",
    "rmse_uwarm_iwarm = root_mean_squared_error(test_uwarm_iwarm['Rating'], pred_uwarm_iwarm)\n",
    "rmse_uwarm_icold = root_mean_squared_error(test_uwarm_icold['Rating'], pred_uwarm_icold)\n",
    "rmse_ucold_iwarm = root_mean_squared_error(test_ucold_iwarm['Rating'], pred_ucold_iwarm)\n",
    "rmse_ucold_icold = root_mean_squared_error(test_ucold_icold['Rating'], pred_ucold_icold)\n",
    "\n",
    "\n",
    "# Evaluate MAE\n",
    "mae_uwarm_iwarm = mean_absolute_error(test_uwarm_iwarm['Rating'], pred_uwarm_iwarm)\n",
    "mae_uwarm_icold = mean_absolute_error(test_uwarm_icold['Rating'], pred_uwarm_icold)\n",
    "mae_ucold_iwarm = mean_absolute_error(test_ucold_iwarm['Rating'], pred_ucold_iwarm)\n",
    "mae_ucold_icold = mean_absolute_error(test_ucold_icold['Rating'], pred_ucold_icold)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print('SVD Results:')\n",
    "print('MSE (warm user, warm item):', mse_uwarm_iwarm)\n",
    "print('RMSE (warm user, warm item):', rmse_uwarm_iwarm)\n",
    "print('MAE (warm user, warm item):', mae_uwarm_iwarm)\n",
    "print('-' * 50)\n",
    "print('MSE (warm user, cold item):', mse_uwarm_icold)\n",
    "print('RMSE (warm user, cold item):', rmse_uwarm_icold)\n",
    "print('MAE (warm user, cold item):', mae_uwarm_icold)\n",
    "print('-' * 50)\n",
    "print('MSE (cold user, warm item):', mse_ucold_iwarm)\n",
    "print('RMSE (cold user, warm item):', rmse_ucold_iwarm)\n",
    "print('MAE (cold user, warm item):', mae_ucold_iwarm)\n",
    "print('-' * 50)\n",
    "print('MSE (cold user, cold item):', mse_ucold_icold)\n",
    "print('RMSE (cold user, cold item):', rmse_ucold_icold)\n",
    "print('MAE (cold user, cold item):', mae_ucold_icold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23fe4d",
   "metadata": {},
   "source": [
    "# Evaluate top-k, since MSE/RMSE/MAE are not really descriptive in cold-start cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02c5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "pred_uwarm_iwarm = pd.read_csv(f'{base_path}\\\\svd\\\\svd_warm_user_warm_item.csv')\n",
    "pred_uwarm_icold = pd.read_csv(f'{base_path}\\\\svd\\\\svd_warm_user_cold_item.csv')\n",
    "pred_ucold_iwarm = pd.read_csv(f'{base_path}\\\\svd\\\\svd_cold_user_warm_item.csv')\n",
    "pred_ucold_icold = pd.read_csv(f'{base_path}\\\\svd\\\\svd_cold_user_cold_item.csv')\n",
    "# Merge predictions with test sets\n",
    "pred_uwarm_iwarm = pd.merge(test_uwarm_iwarm, pred_uwarm_iwarm, on='RatingID', how='inner', suffixes=('', '_pred'))\n",
    "pred_uwarm_icold = pd.merge(test_uwarm_icold, pred_uwarm_icold, on='RatingID', how='inner', suffixes=('', '_pred'))\n",
    "pred_ucold_iwarm = pd.merge(test_ucold_iwarm, pred_ucold_iwarm, on='RatingID', how='inner', suffixes=('', '_pred'))\n",
    "pred_ucold_icold = pd.merge(test_ucold_icold, pred_ucold_icold, on='RatingID', how='inner', suffixes=('', '_pred'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08153634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Rank and Rank_pred columns\n",
    "\n",
    "# Warm user, warm item\n",
    "pred_uwarm_iwarm[\"Rank\"] = pred_uwarm_iwarm.groupby(\"UserID\")[\"Rating\"].rank(method=\"first\", ascending=False)\n",
    "pred_uwarm_iwarm[\"Rank_pred\"] = pred_uwarm_iwarm.groupby(\"UserID\")[\"Rating_pred\"].rank(method=\"first\", ascending=False)\n",
    "# Warm user, cold item\n",
    "pred_uwarm_icold[\"Rank\"] = pred_uwarm_icold.groupby(\"UserID\")[\"Rating\"].rank(method=\"first\", ascending=False)\n",
    "pred_uwarm_icold[\"Rank_pred\"] = pred_uwarm_icold.groupby(\"UserID\")[\"Rating_pred\"].rank(method=\"first\", ascending=False)\n",
    "# Cold user, warm item\n",
    "pred_ucold_iwarm[\"Rank\"] = pred_ucold_iwarm.groupby(\"UserID\")[\"Rating\"].rank(method=\"first\", ascending=False)\n",
    "pred_ucold_iwarm[\"Rank_pred\"] = pred_ucold_iwarm.groupby(\"UserID\")[\"Rating_pred\"].rank(method=\"first\", ascending=False)\n",
    "# Cold user, cold item\n",
    "pred_ucold_icold[\"Rank\"] = pred_ucold_icold.groupby(\"UserID\")[\"Rating\"].rank(method=\"first\", ascending=False)\n",
    "pred_ucold_icold[\"Rank_pred\"] = pred_ucold_icold.groupby(\"UserID\")[\"Rating_pred\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "# Calculate Relevance\n",
    "pred_uwarm_iwarm[\"Relevance\"] = pred_uwarm_iwarm[\"Rating\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "pred_uwarm_icold[\"Relevance\"] = pred_uwarm_icold[\"Rating\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "pred_ucold_iwarm[\"Relevance\"] = pred_ucold_iwarm[\"Rating\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "pred_ucold_icold[\"Relevance\"] = pred_ucold_icold[\"Rating\"].apply(lambda x: 1 if x >= 3.5 else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50e412a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_topk_fast(df, k=10):\n",
    "    # Pre-sort so top-k is at the top per user\n",
    "    df = df.sort_values(['UserID', 'Rank_pred'], ascending=[True, True])\n",
    "\n",
    "    # Assign group index per row (unique integer per user)\n",
    "    user_index, user_pos = np.unique(df['UserID'], return_inverse=True)\n",
    "\n",
    "    # Count items per user\n",
    "    user_counts = np.bincount(user_pos)\n",
    "    user_offsets = np.zeros(len(df), dtype=int)\n",
    "    np.add.at(user_offsets, np.cumsum(user_counts)[:-1], 1)\n",
    "    user_offsets = np.cumsum(user_offsets)\n",
    "\n",
    "    # Mask to keep only top-k per user\n",
    "    df['row_number'] = df.groupby('UserID').cumcount()\n",
    "    topk_df = df[df['row_number'] < k].copy()\n",
    "\n",
    "    # Precision@k\n",
    "    precision = topk_df['Relevance'].groupby(topk_df['UserID']).mean().mean()\n",
    "\n",
    "    # Recall@k\n",
    "    relevant_per_user = df.groupby('UserID')['Relevance'].sum()\n",
    "    hits_per_user = topk_df.groupby('UserID')['Relevance'].sum()\n",
    "    recall = (hits_per_user / relevant_per_user).fillna(0).mean()\n",
    "\n",
    "    # HitRate@k\n",
    "    hits = (hits_per_user > 0).astype(int)\n",
    "    hit_rate = hits.mean()\n",
    "\n",
    "    # MAP@k\n",
    "    def map_at_k_per_user(x):\n",
    "        rels = x['Relevance'].values\n",
    "        precisions = [(rels[:i + 1].sum() / (i + 1)) for i in range(len(rels)) if rels[i]]\n",
    "        return np.mean(precisions) if precisions else 0\n",
    "    mapk = topk_df.groupby('UserID').apply(map_at_k_per_user, include_groups=False).mean()\n",
    "\n",
    "    # nDCG@k\n",
    "    def dcg(rels):\n",
    "        return np.sum(rels / np.log2(np.arange(2, len(rels) + 2)))\n",
    "    def ndcg_per_user(x):\n",
    "        dcg_val = dcg(x['Relevance'].values)\n",
    "        ideal = x.sort_values('Relevance', ascending=False).head(k)\n",
    "        idcg_val = dcg(ideal['Relevance'].values)\n",
    "        return dcg_val / idcg_val if idcg_val > 0 else 0\n",
    "    ndcg = topk_df.groupby('UserID').apply(ndcg_per_user, include_groups=False).mean()\n",
    "\n",
    "    return {\n",
    "        f'Precision@{k}': precision,\n",
    "        f'Recall@{k}': recall,\n",
    "        f'HitRate@{k}': hit_rate,\n",
    "        f'MAP@{k}': mapk,\n",
    "        f'nDCG@{k}': ndcg\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41b2a337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on warm user, warm item at top 10:\n",
      "Precision@10: 0.8819\n",
      "Recall@10: 0.9238\n",
      "HitRate@10: 0.9526\n",
      "MAP@10: 0.9261\n",
      "nDCG@10: 0.9362\n",
      "--------------------------------------------------\n",
      "Evaluation on warm user, cold item at top 10:\n",
      "Precision@10: 0.8389\n",
      "Recall@10: 0.8605\n",
      "HitRate@10: 0.8605\n",
      "MAP@10: 0.8479\n",
      "nDCG@10: 0.8515\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, warm item at top 10:\n",
      "Precision@10: 0.8192\n",
      "Recall@10: 0.9233\n",
      "HitRate@10: 0.9929\n",
      "MAP@10: 0.9164\n",
      "nDCG@10: 0.9489\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, cold item at top 10:\n",
      "Precision@10: 0.7089\n",
      "Recall@10: 0.7210\n",
      "HitRate@10: 0.7210\n",
      "MAP@10: 0.7139\n",
      "nDCG@10: 0.7159\n",
      "--------------------------------------------------\n",
      "Evaluation on warm user, warm item at top 20:\n",
      "Precision@20: 0.8795\n",
      "Recall@20: 0.9437\n",
      "HitRate@20: 0.9526\n",
      "MAP@20: 0.9253\n",
      "nDCG@20: 0.9360\n",
      "--------------------------------------------------\n",
      "Evaluation on warm user, cold item at top 20:\n",
      "Precision@20: 0.8389\n",
      "Recall@20: 0.8605\n",
      "HitRate@20: 0.8605\n",
      "MAP@20: 0.8479\n",
      "nDCG@20: 0.8515\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, warm item at top 20:\n",
      "Precision@20: 0.8112\n",
      "Recall@20: 0.9761\n",
      "HitRate@20: 0.9930\n",
      "MAP@20: 0.9134\n",
      "nDCG@20: 0.9483\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, cold item at top 20:\n",
      "Precision@20: 0.7089\n",
      "Recall@20: 0.7210\n",
      "HitRate@20: 0.7210\n",
      "MAP@20: 0.7139\n",
      "nDCG@20: 0.7159\n",
      "--------------------------------------------------\n",
      "Evaluation on warm user, warm item at top 50:\n",
      "Precision@50: 0.8784\n",
      "Recall@50: 0.9514\n",
      "HitRate@50: 0.9526\n",
      "MAP@50: 0.9250\n",
      "nDCG@50: 0.9360\n",
      "--------------------------------------------------\n",
      "Evaluation on warm user, cold item at top 50:\n",
      "Precision@50: 0.8389\n",
      "Recall@50: 0.8605\n",
      "HitRate@50: 0.8605\n",
      "MAP@50: 0.8479\n",
      "nDCG@50: 0.8515\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, warm item at top 50:\n",
      "Precision@50: 0.8088\n",
      "Recall@50: 0.9909\n",
      "HitRate@50: 0.9930\n",
      "MAP@50: 0.9126\n",
      "nDCG@50: 0.9482\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, cold item at top 50:\n",
      "Precision@50: 0.7089\n",
      "Recall@50: 0.7210\n",
      "HitRate@50: 0.7210\n",
      "MAP@50: 0.7139\n",
      "nDCG@50: 0.7159\n",
      "--------------------------------------------------\n",
      "Evaluation on warm user, warm item at top 100:\n",
      "Precision@100: 0.8782\n",
      "Recall@100: 0.9524\n",
      "HitRate@100: 0.9526\n",
      "MAP@100: 0.9250\n",
      "nDCG@100: 0.9360\n",
      "--------------------------------------------------\n",
      "Evaluation on warm user, cold item at top 100:\n",
      "Precision@100: 0.8389\n",
      "Recall@100: 0.8605\n",
      "HitRate@100: 0.8605\n",
      "MAP@100: 0.8479\n",
      "nDCG@100: 0.8515\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, warm item at top 100:\n",
      "Precision@100: 0.8084\n",
      "Recall@100: 0.9927\n",
      "HitRate@100: 0.9930\n",
      "MAP@100: 0.9125\n",
      "nDCG@100: 0.9481\n",
      "--------------------------------------------------\n",
      "Evaluation on cold user, cold item at top 100:\n",
      "Precision@100: 0.7089\n",
      "Recall@100: 0.7210\n",
      "HitRate@100: 0.7210\n",
      "MAP@100: 0.7139\n",
      "nDCG@100: 0.7159\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "ks = [10, 20, 50, 100]\n",
    "# Evaluate for different k values\n",
    "for k in ks:\n",
    "    results = {}\n",
    "    results['warm user, warm item'] = evaluate_topk_fast(pred_uwarm_iwarm, k=k)\n",
    "    results['warm user, cold item'] = evaluate_topk_fast(pred_uwarm_icold, k=k)\n",
    "    results['cold user, warm item'] = evaluate_topk_fast(pred_ucold_iwarm, k=k)\n",
    "    results['cold user, cold item'] = evaluate_topk_fast(pred_ucold_icold, k=k)\n",
    "\n",
    "    # Print evaluation results\n",
    "    for case, metrics in results.items():\n",
    "        print(f\"Evaluation on {case} at top {k}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        print('-' * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
